{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어\\_태그\\_T/F(받침여부)_원단어의발음\n",
    "### https://docs.google.com/spreadsheets/d/1-9blXKjtjeKZqsf4NzHeYJCrr49-nXeRF6D80udfcwY/edit#gid=4\n",
    "# 명사 NNG, 동사 VV, 형용사 VA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abys\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.corpora import Dictionary\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import mglearn\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import gc\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class SB_Word2Vec():    \n",
    "    \n",
    "    def __init__(self, morph_list):\n",
    "        self.dct = Dictionary(morph_list)\n",
    "        self.corpus = [self.dct.doc2bow(line) for line in morph_list]\n",
    "        self.build_Word2Vec(morph_list)\n",
    "    \n",
    "    def make_Word2Vec(self, morph_list, size=50, window=2, min_count=10, iteration=100):\n",
    "        self.em = Word2Vec(morph_list, size=size, window=window, min_count=min_count, iter=iteration)\n",
    "        self.em_vocab = list(self.em.wv.vocab.keys())\n",
    "        self.em_vocab_dic = {word:idx for idx, word in enumerate(self.em_vocab)}\n",
    "\n",
    "    def make_Word2Sen_matrix(self): \n",
    "        vocab_size = len(self.em_vocab)\n",
    "        self.sen_matrix = np.zeros((len(self.corpus), vocab_size))\n",
    "        for idx, row in enumerate(self.sen_matrix):\n",
    "            for idx2, frequency in self.corpus[idx]:\n",
    "                    if self.dct[idx2] in self.em_vocab:\n",
    "                        self.sen_matrix[idx][self.em_vocab_dic[self.dct[idx2]]] = frequency                \n",
    "        self.sim_matrix = np.zeros((vocab_size, vocab_size))\n",
    "        for idx, w1 in enumerate(self.em_vocab):\n",
    "            for idx2, w2 in enumerate(self.em_vocab):\n",
    "                self.sim_matrix[idx][idx2] =  self.em.wv.similarity(w1, w2)\n",
    "\n",
    "        self.word2sen_matrix = np.dot(self.sim_matrix, np.transpose(self.sen_matrix))\n",
    "\n",
    "        return self.word2sen_matrix\n",
    "\n",
    "    def get_sim_sen(self, keyword, main_text, number=1):\n",
    "        self.sim_sen_index = np.argsort(self.word2sen_matrix[self.em_vocab_dic[keyword]])\n",
    "        self.most_sim_sen_index = np.argmax(self.word2sen_matrix[self.em_vocab_dic[keyword]])\n",
    "        index_list = self.sim_sen_index.reshape((-1,)).tolist()\n",
    "        index_list.reverse()\n",
    "        \n",
    "        for idx, i in enumerate(index_list[:number]):\n",
    "            print(str(idx + 1))\n",
    "            print(main_text[i])\n",
    "        return index_list\n",
    "    \n",
    "    def build_Word2Vec(self, morph_list):\n",
    "        self.make_Word2Vec(morph_list)\n",
    "        self.make_Word2Sen_matrix()\n",
    "        \n",
    "        \n",
    "class SB_LDA():\n",
    "\n",
    "    def make_lda(self, morph_joined, ntopic=10, learning_method='batch', max_iter=25, random_state=0, n_words=20):        \n",
    "        self.vect = CountVectorizer(max_features=10000, max_df=.15)\n",
    "        self.X = self.vect.fit_transform(morph_joined)\n",
    "        self.lda = LatentDirichletAllocation(n_components=ntopic, learning_method=learning_method, max_iter=max_iter, random_state=random_state)\n",
    "        self.document_topics = self.lda.fit_transform(self.X)\n",
    "        self.sorting = np.argsort(self.lda.components_, axis=1)[:, ::-1]\n",
    "        self.feature_names = np.array(self.vect.get_feature_names())\n",
    "        mglearn.tools.print_topics(topics=range(ntopic), feature_names=self.feature_names, sorting=self.sorting, topics_per_chunk=5, n_words=n_words)\n",
    "\n",
    "    def related_doc(self, main_text_list, topic_index, number=10):\n",
    "        category = np.argsort(self.document_topics[:, topic_index])[::-1]\n",
    "        related_docs = []\n",
    "        for i in category[:number]:\n",
    "            print(i)\n",
    "            print(main_text_list[i] + \".\\n\")\n",
    "            related_docs.append((i, main_text_list[i]))\n",
    "        return related_docs\n",
    "\n",
    "class SB_Tfidf():    \n",
    "    \n",
    "    def __init__(self, list_morph_merged):\n",
    "        self.list_morph_merged = list_morph_merged\n",
    "        self.dct = Dictionary(self.list_morph_merged)\n",
    "        self.corpus = [self.dct.doc2bow(line) for line in self.list_morph_merged]\n",
    "\n",
    "    def get_tfidf(self):       \n",
    "        self.model = TfidfModel(self.corpus)\n",
    "        self.tfidf = []\n",
    "        for i in self.corpus:\n",
    "             self.tfidf.append(sorted(self.model[i], key = lambda x: x[1], reverse=True))\n",
    "        self.tfidf_hangul = []\n",
    "        for idx1, i in enumerate(self.tfidf):\n",
    "            self.tfidf_hangul.append([(self.dct[j[0]], j[1]) for j in i])        \n",
    "        \n",
    "        return self.tfidf_hangul\n",
    "    \n",
    "def frequency(merged):\n",
    "    word_count = Counter(merged)\n",
    "    word_count2 = []\n",
    "    for i in word_count:\n",
    "        word_count2.append((i, word_count[i]))\n",
    "    word_count2 = sorted(word_count2, key=lambda x: x[1], reverse = True)\n",
    "    return word_count2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Social_analysis():\n",
    "    \n",
    "    non_bmp_map = dict.fromkeys(range(0x10000, sys.maxunicode + 1), 0xfffd)\n",
    "    syn_dic = {}\n",
    "    theme_dic = {}\n",
    "    del_list = []\n",
    "    ngram_dic = {}\n",
    "    exception_list=['맛', '밥', '물', '몸', '없', '있', '싫', '달', '굳', '굿', '속']\n",
    "    \n",
    "    default_dic_path = 'Data/custom_dic.csv'\n",
    "    replace_dic = 'Data/replace_dic.csv'\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.mecab = Mecab()\n",
    "        try:\n",
    "            self.load_dictionary()\n",
    "        except Exception as e:\n",
    "            print('dictionary error\\n', e)\n",
    "    def load_dictionary(self, mode='default'):\n",
    "        path = self.default_dic_path\n",
    "        self.dic_df = pd.read_csv(path, encoding='cp949')\n",
    "        for i in range(len(self.dic_df)):\n",
    "            key = self.dic_df.loc[i,'key']\n",
    "            value = self.dic_df.loc[i, 'value']\n",
    "            syn = self.dic_df.loc[i, 'syn']\n",
    "            theme = self.dic_df.loc[i, 'theme']\n",
    "\n",
    "            if pd.isna(value):\n",
    "                print('Need key & value')\n",
    "                return\n",
    "\n",
    "            self.ngram_dic[key] = value\n",
    "            \n",
    "            if not pd.isna(theme):\n",
    "                value = value.split('_')[0]\n",
    "                if not pd.isna(syn):\n",
    "                    self.syn_dic[value] = syn\n",
    "                    self.theme_dic[syn] = theme\n",
    "                else:\n",
    "                    self.theme_dic[value.split('_')[0]] = theme\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    def DB_to_table(self, DBname='intake', keyword='intake'):\n",
    "        import pymssql\n",
    "        import pandas.io.sql as pdsql\n",
    "        import pandas as pd\n",
    "        self.query = \\\n",
    "        \"\"\"\n",
    "        SELECT user_id, created_at, main_text, hashtags, comments, likes, current_url FROM instaPosting WHERE keyword = '{}'\n",
    "        \"\"\".format(keyword)\n",
    "        conn = pymssql.connect(\"intakedb.c63elkxbiwfc.us-east-2.rds.amazonaws.com:1433\", \"gh\", \"ghintake\", DBname)\n",
    "        self.df = pdsql.read_sql_query(self.query, con=conn)\n",
    "        # df['main_text'] = df.main_text.apply(lambda x: x.replace('#',' ').translate(self.non_bmp_map))\n",
    "        # df['created_at'] = df.created_at.apply(lambda x: x.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        conn.close()       \n",
    "    \n",
    "    def pickle_to_table(self, filename, columns=['user_id', 'created_at', 'main_text', 'hashtags', 'comments', 'likes', 'current_url']):\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        data = data[1:]\n",
    "        for idx, i in enumerate(data):\n",
    "            data[idx][2] = i[2].replace('#', ' ').translate(self.non_bmp_map)\n",
    "            data[idx][3] = '/'.join(i[3])\n",
    "            data[idx][4] = '/'.join(i[4])\n",
    "        self.df = pd.DataFrame(np.array(data), columns=['user_id', 'created_at', 'main_text', 'hashtags', 'comments', 'likes', 'current_url'])\n",
    "  \n",
    "    def hashtags_split(self, hashtags):        \n",
    "        hashtags_split = []\n",
    "        for i in hashtags:\n",
    "            hashtags_split.append(i.split('/'))\n",
    "        \n",
    "        hashtags_list = []\n",
    "        \n",
    "        for i in hashtags_split:\n",
    "            temp = []\n",
    "            for j in i:\n",
    "                if self.isHangul(j):\n",
    "                    t_hashtags = j.translate(self.non_bmp_map)\n",
    "                    temp.append(t_hashtags)\n",
    "            hashtags_list.append(temp)\n",
    "        self.hashtags_list = hashtags_list\n",
    "        \n",
    "        return hashtags_list\n",
    "\n",
    "    \n",
    "    def add_dictionary(self, *tokenized_list):\n",
    "        origin_df = 1\n",
    "        try:\n",
    "            origin_df = pd.read_csv(\"C:\\\\mecab\\\\user-dic\\\\intake_dic.csv\", encoding='utf-8', header=None)\n",
    "        except:\n",
    "            print('No default intake_dic')\n",
    "        keyword_list = []   \n",
    "        for i in tokenized_list:\n",
    "            if type(i) == list:\n",
    "                for j in i:\n",
    "                    j = j.split('_')\n",
    "                    temp = [j[0],'' ,'' ,'' ,j[1],'*',j[2], j[3],'*','*','*','*','*']\n",
    "                    keyword_list.append(temp)\n",
    "            else:\n",
    "                i = i.split('_')\n",
    "                temp = [i[0],'','','',i[1],'*',i[2], i[3], '*','*','*','*','*']\n",
    "                keyword_list.append(temp)\n",
    "\n",
    "\n",
    "        keyword_df = pd.DataFrame(keyword_list)\n",
    "        print(type(origin_df))\n",
    "        if type(origin_df) != int:\n",
    "            keyword_df = pd.concat((origin_df, keyword_df), ignore_index=True)\n",
    "        else: \n",
    "            print('a')\n",
    "            pass\n",
    "        print(keyword_df.shape)\n",
    "\n",
    "        keyword_df.to_csv(\"C:\\\\mecab\\\\user-dic\\\\intake_dic.csv\", encoding='utf-8',index=None, header=False)    \n",
    "\n",
    "                \n",
    "    def ngram(self, parsed_list):\n",
    "        ngram_list = []        \n",
    "        adjustment = 0\n",
    "        # 단어_tag의 리스트\n",
    "\n",
    "        for idx in range(len(parsed_list)):\n",
    "            idx2 = idx + adjustment\n",
    "\n",
    "            if (idx2+self.ngram_size) > (len(parsed_list)):\n",
    "                ngram_list.extend(parsed_list[idx2:])\n",
    "                break\n",
    "            n_filter = tuple(parsed_list[idx2: idx2 + self.ngram_size])\n",
    "            key = ''.join([k.split('_')[0] for k in n_filter])\n",
    "            if key in self.ngram_dic:\n",
    "                ngram_list.append(self.ngram_dic[key])\n",
    "                adjustment += (self.ngram_size - 1)\n",
    "            else:\n",
    "                ngram_list.append(n_filter[0])\n",
    "\n",
    "        if self.ngram_size <= 1:\n",
    "            return ngram_list\n",
    "        else:\n",
    "            self.ngram_size -= 1\n",
    "            return self.ngram(ngram_list)       \n",
    "\n",
    "        \n",
    "    def morph_pos(self, text_list,  mode='list'):\n",
    "        \n",
    "        morph_list = []\n",
    "        \n",
    "        for j in text_list:\n",
    "            parsed = self.mecab.pos(j)\n",
    "            temp = []\n",
    "            for i in parsed:\n",
    "                if self.isHangul(i[0]):\n",
    "                    temp.append('{}_{}'.format(i[0], i[1]))\n",
    "                else: pass#print('{} 한글이 아님.'.format(i[0]))\n",
    "\n",
    "            self.ngram_size = 6\n",
    "            morph_list.append(self.ngram(temp))\n",
    "            \n",
    "        self.df['morph_list'] = morph_list\n",
    "        \n",
    "        return morph_list\n",
    "\n",
    "    def filter_words(self, parsed_list, mode='syn'):\n",
    "        # 1차원 리스트를 받음.\n",
    "        \n",
    "        if mode == 'None':\n",
    "            return\n",
    "        \n",
    "        changed_list = list(map(lambda x: self.syn_dic.get(x, x) , parsed_list))\n",
    "        deleted_list = list(filter(lambda x: x not in self.del_list, changed_list))\n",
    "        \n",
    "        if mode == 'theme':\n",
    "            theme_list = list(map(lambda x: self.theme_dic.get(x, x) , deleted_list))\n",
    "            return theme_list            \n",
    "        else:\n",
    "            return deleted_list\n",
    "\n",
    "    def set_with_order(self, sequence):\n",
    "        seen = set()\n",
    "        result = [x for x in sequence if not (x in seen or seen.add(x))]\n",
    "        return result\n",
    "\n",
    "    \n",
    "    def pos_extractor(self, parsed, mode = 'list', degree = 'syn'):\n",
    "        \n",
    "        \n",
    "        noun_list = []\n",
    "        adj_list = []\n",
    "        verb_list = []\n",
    "        nav_list = []\n",
    "        total_list = [nav_list, noun_list, adj_list, verb_list]\n",
    "        \n",
    "        for j in parsed:\n",
    "            nav_temp = []\n",
    "            n_temp = []\n",
    "            adj_temp = []\n",
    "            verb_temp = []\n",
    "            temp_list = [nav_temp,  n_temp, adj_temp, verb_temp]\n",
    "            \n",
    "            for i in j:\n",
    "                i = i.split('_')\n",
    "                if self.isHangul(i[0]):\n",
    "                    if (len(i[0]) > 1) or (i[0] in self.exception_list):                        \n",
    "                        if 'NN' in i[1]:\n",
    "                            n_temp.append(i[0])\n",
    "                            nav_temp.append(i[0])\n",
    "                        elif 'VV'in i[1]:\n",
    "                            adj_temp.append(i[0])\n",
    "                            nav_temp.append(i[0])\n",
    "                        elif 'VA' in i[1]:\n",
    "                            verb_temp.append(i[0])\n",
    "                            nav_temp.append(i[0])\n",
    "                    else: pass\n",
    "                        #print('{} 제외'.format(i[0]))\n",
    "                else: pass#print('{} 한글이 아님.'.format(i[0]))\n",
    "\n",
    "            \n",
    "            for idx, li in enumerate(total_list):\n",
    "                if mode == 'list':\n",
    "                    li.append(temp_list[idx])\n",
    "                elif mode == 'set':\n",
    "                    li.append(self.set_with_order(self.filter_words(temp_list[idx], degree)))\n",
    "                else:\n",
    "                    print('Check mode')\n",
    "                    return\n",
    "            \n",
    "            \n",
    "        columns=['nav_list', 'noun_list', 'adj_list', 'verb_list']\n",
    "        for i in  zip(columns, total_list):\n",
    "            self.df[i[0]] = i[1]\n",
    "            \n",
    "        #return nav_list, noun_list, adj_list, verb_list # tuple(map(lambda x: [j.split('_')[0] for j in x], [nav_list, noun_list, adj_list, verb_list]))\n",
    "\n",
    "    \n",
    "    def merge_list(self, tokenized_list):\n",
    "        return [j for i in tokenized_list for j in i]\n",
    "\n",
    "    \n",
    "    def join_list(self, tokenized_list):\n",
    "        joined_list = []\n",
    "        for idx, i in enumerate(tokenized_list):\n",
    "            joined_list.append(\" \".join(i))\n",
    "        return joined_list\n",
    " \n",
    "    def split_list(self, untokenized_list):\n",
    "        hashtag_splited = []\n",
    "        for idx, i in enumerate(untokenized):\n",
    "            hashtag_splited.append(i.split('/'))\n",
    "            return hastag_splited\n",
    "        \n",
    "    '''    \n",
    "    def join_underbar(self, morph_list):\n",
    "\n",
    "        all_list = []\n",
    "        post_list=[]\n",
    "        for i in morph_list:\n",
    "            for j in i:\n",
    "                post_list.append(j[0]+'_'+j[1])\n",
    "            all_list.append([(' , ').join(post_list)])\n",
    "            post_list=[] \n",
    "        all_list=np.array(all_list)\n",
    "        \n",
    "        return all_list'''\n",
    "\n",
    "    def word_substitute(self, dataset, sublist):\n",
    "        dataset = copy.deepcopy(dataset)\n",
    "        sub_book = dict()\n",
    "        for i in sublist:\n",
    "            for j in i['sub_words']:\n",
    "                sub_book[j] = i['main']\n",
    "        gc.collect()\n",
    "        for n, i in enumerate(dataset):\n",
    "            dataset[n] = [sub_book.get(item,item) for item in i]\n",
    "\n",
    "        del sub_book\n",
    "        gc.collect()\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    def word_delete(self, dataset, del_list):\n",
    "        dataset = copy.deepcopy(dataset)\n",
    "\n",
    "        for n, line in enumerate(dataset):\n",
    "             dataset[n] = [i for i in line if i not in del_list]\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    \n",
    "    def isHangul(self, text):\n",
    "        encText = text\n",
    "        hanCount = len(re.findall(u'[\\u3130-\\u318F\\uAC00-\\uD7A3]+', encText))\n",
    "        return hanCount > 0\n",
    "    \n",
    "    def convert_list(self, *tokenized_list):\n",
    "        input_length = len(tokenized_list)\n",
    "        lists = [[] for i in range(input_length)]\n",
    "\n",
    "        for idx, li in enumerate(tokenized_list):\n",
    "            for j in li:\n",
    "                lists[idx].append(['/'.join(j)])\n",
    "\n",
    "        converted_array = np.array(lists[0])\n",
    "        for idx in range(input_length):\n",
    "            try:\n",
    "                converted_array = np.concatenate((converted_array, lists[idx + 1]), axis=1)\n",
    "            except Exception as e:\n",
    "                print(e,'끝')\n",
    "\n",
    "        return converted_array\n",
    "\n",
    "    def make_df(self, start_array, converted_array, end_array, columns=['user_id', 'created_at', 'main_text', 'morph_list', 'nav_list', 'noun_list', 'adj_list', 'verb_list', 'hashtags', 'comments', 'likes', 'current_url']):         \n",
    "        df = pd.DataFrame(np.hstack((start_array, converted_array, end_array)), index=None, columns=columns)\n",
    "        return df\n",
    "    \n",
    "    # 키워드 리스트 중 하나라도 있는 경우\n",
    "    def word_check_or(self, text, keywords):\n",
    "        if any(word in text for word in keywords):\n",
    "            return 1\n",
    "        else: return 0\n",
    "\n",
    "    # 키워드 리스트에 있는 단어가 모두 있는 경우\n",
    "    def word_check_and(self, text, keywords):\n",
    "        if all(word in text for word in keywords):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "    def word_check(self, method, keywords, df, column_name = 'main_text',filter_TF=True):\n",
    "        \n",
    "        filter_TF = 1 if filter_TF == True else 0\n",
    "        if method == 'and':\n",
    "            df['flags'] = df[column_name].apply(lambda x: self.word_check_and(x, keywords))\n",
    "            return df.loc[df['flags'] == filter_TF]\n",
    "\n",
    "        elif method == 'or':\n",
    "            df['flags'] = df[column_name].apply(lambda x: self.word_check_or(x, keywords))\n",
    "            return df.loc[df['flags'] == filter_TF]\n",
    "        \n",
    "        else:\n",
    "            print('Select method, and/or')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake = Social_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pickle이나 db에서 데이터를 불러오는 순간 self.df 로 저장이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "intake.pickle_to_table('Data/intake_list.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake.df = intake.word_check('or', ['자동차', '흡기', '배기','도어락'], intake.df, 'hashtags', False)\n",
    "intake.df = intake.word_check('or', ['intakefoods', \n",
    "                                     'dameulstudio', \n",
    "                                     '_.ddo2', \n",
    "                                     '__scarlett.k', \n",
    "                                     '0.8l_korea', \n",
    "                                     'jiseung86', \n",
    "                                     'untactmarket', \n",
    "                                     'hyorin_papa', \n",
    "                                     'eighty4u', \n",
    "                                     'redamethyst3', \n",
    "                                     'hellovenus101', \n",
    "                                     'dearblossom.cake'], intake.df, 'user_id', False)\n",
    "intake.df = intake.word_check('or', ['참여방법'], intake.df, 'main_text', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- temp = intake.df.loc[intake.df['main_text'].str.contains('맛') & intake.df['main_text'].str.contains('모닝죽') & intake.df['main_text'].str.contains('있')]['']\n",
    "- temp.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## morph_pos 함수를 실행하면 self.df['morph_list'] 에 저장된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "intake.morph_list = intake.morph_pos(intake.df['main_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## pos_extractor 함수는 더 이상 return을 하지 않고, self.df에 컬럼을 추가한다.\n",
    "- 'nav_list', 'noun_list', 'adj_list', 'verb_list' 컬럼\n",
    "\n",
    "## mode는 'list', 'set'모드가 있다. default는 list이다.\n",
    "- list mode일 때에는 중복되는 단어를 제거하지 않는다. \n",
    "- 'set'모드 일 때는 중복된 단어를 제거한다.\n",
    "\n",
    "## degree는 'none', 'syn', 'theme'이 있다. syn이 디폴트이다.\n",
    "- 'none'은 형태소 분석까지 기준\n",
    "- 'syn'은 csv컬럼기준, syn까지의 단어를 바꿔준다.\n",
    "- theme 은 theme컬럼까지."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake.pos_extractor(intake.morph_list, mode='list', degree='none')\n",
    "intake.df['nav_list'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake.pos_extractor(intake.morph_list, mode='set', degree='none')\n",
    "intake.df['nav_list'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake.pos_extractor(intake.morph_list, mode='set', degree='theme')\n",
    "intake.df['nav_list'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예전처럼 raw_list를 넣을 필요 없이 그냥 df[컬럼명] 으로 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake.noun_merged = intake.merge_list(intake.df['noun_list'])\n",
    "intake.noun_frequency = frequency(intake.noun_merged)\n",
    "pprint(intake.noun_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake.df['nav_list'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake.noun_joined = intake.join_list(intake.df['noun_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake.LDA = SB_LDA()\n",
    "intake.LDA.make_lda(intake.nav_joined, ntopic=5, learning_method='batch', max_iter=25, random_state=0, n_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "단어_태그_T/F(받침여부)_원단어의발음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_dic = '''모닝죽,,,,NNG,*,T,모닝죽,*,*,*,*,*\n",
    "인테이크,,,,NNG,*,F,인테이크,*,*,*,*,*\n",
    "꿀고구마,,,,NNG,*,F,꿀고구마,*,*,*,*,*\n",
    "한끼,,,,NNG,*,F,한끼,*,*,*,*,*\n",
    "밀스라이트,,,,NNG,*,F,밀스라이트,*,*,*,*,*\n",
    "식단인증,,,,NNG,*,F,식단인증,*,*,*,*,*\n",
    "직장인,,,,NNG,*,T,직장인,*,*,*,*,*\n",
    "스타그램,,,,NNG,*,T,스타그램,*,*,*,*,*\n",
    "귀리우유,,,,NNG,*,F,귀리우유,*,*,*,*,*\n",
    "텐바이텐,,,,NNG,*,T,텐바이텐,*,*,*,*,*\n",
    "텐텐쇼퍼,,,,NNG,*,F,텐텐쇼퍼,*,*,*,*,*\n",
    "간편식,,,,NNG,*,T,간편식,*,*,*,*,*\n",
    "1일1식,,,,NNG,*,T,1일1식,*,*,*,*,*\n",
    "카페라떼,,,,NNG,*,F,카페라떼,*,*,*,*,*\n",
    "벨벳초콜렛,,,,NNG,*,T,벨벳초콜렛,*,*,*,*,*\n",
    "혼밥,,,,NNG,*,T,혼밥,*,*,*,*,*\n",
    "곤약현미밥,,,,NNG,*,T,곤약현미밥,*,*,*,*,*\n",
    "버터치킨커리,,,,NNG,*,F,버터치킨커리,*,*,*,*,*\n",
    "유지어터,,,,NNG,*,F,유지어터,*,*,*,*,*\n",
    "치킨커리,,,,NNG,*,F,치킨커리,*,*,*,*,*\n",
    "버터치킨,,,,NNG,*,F,버터치킨,*,*,*,*,*\n",
    "밀스칩,,,,NNG,*,T,밀스칩,*,*,*,*,*\n",
    "미래지향,,,,NNG,*,F,미래지향,*,*,*,*,*\n",
    "유통기한,,,,NNG,*,T,유통기한,*,*,*,*,*\n",
    "고농축,,,,NNG,*,T,고농축,*,*,*,*,*\n",
    "단백질,,,,NNG,*,T,단백질,*,*,*,*,*\n",
    "밀스드링크딸기,,,,NNG,*,F,밀스드링크딸기,*,*,*,*,*\n",
    "프레시모닝,,,,NNG,*,T,프레시모닝,*,*,*,*,*\n",
    "대체,,,,NNG,*,F,대체,*,*,*,*,*\n",
    "클랜즈,,,,NNG,*,F,클랜즈,*,*,*,*,*\n",
    "와디즈,,,,NNG,*,F,와디즈,*,*,*,*,*\n",
    "얼리버드,,,,NNG,*,F,얼리버드,*,*,*,*,*\n",
    "까페라떼,,,,NNG,*,F,까페라떼,*,*,*,*,*\n",
    "로얄밀크티,,,,NNG,*,F,로얄밀크티,*,*,*,*,*\n",
    "곤약젤리,,,,NNG,*,F,곤약젤리,*,*,*,*,*\n",
    "모닝채소,,,,NNG,*,F,모닝채소,*,*,*,*,*\n",
    "콜드프레스,,,,NNG,*,F,콜드프레스,*,*,*,*,*\n",
    "하루야채,,,,NNG,*,F,하루야채,*,*,*,*,*\n",
    "씨리얼,,,,NNG,*,T,씨리얼,*,*,*,*,*\n",
    "콜드부르,,,,NNG,*,F,콜드부르,*,*,*,*,*\n",
    "스타벅스,,,,NNG,*,F,스타벅스,*,*,*,*,*\n",
    "밀스드링크,,,,NNG,*,F,밀스드링크,*,*,*,*,*\n",
    "존맛,,,,VA,*,T,존맛,*,*,*,*,*\n",
    "아워홈,,,,NNG,*,T,아워홈,*,*,*,*,*\n",
    "모닝바게트,,,,NNG,*,F,모닝바게트,*,*,*,*,*\n",
    "모닝귀리,,,,NNG,*,F,모닝귀리,*,*,*,*,*\n",
    "스윗그레인,,,,NNG,*,T,스윗그레인,*,*,*,*,*\n",
    "그릭요거트,,,,NNG,*,F,그릭요거트,*,*,*,*,*\n",
    "팥톡스,,,,NNG,*,F,팥톡스,*,*,*,*,*\n",
    "식이섬유,,,,NNG,*,F,식이섬유,*,*,*,*,*\n",
    "베지밀,,,,NNG,*,T,베지밀,*,*,*,*,*\n",
    "그레인하프,,,,NNG,*,F,그레인하프,*,*,*,*,*\n",
    "코코넛하프,,,,NNG,*,F,코코넛하프,*,*,*,*,*\n",
    "식이조절,,,,NNG,*,T,식이조절,*,*,*,*,*\n",
    "줌마,,,,NNG,*,F,줌마,*,*,*,*,*\n",
    "매일유업,,,,NNG,*,T,매일유업,*,*,*,*,*\n",
    "볶음콩,,,,NNG,*,T,볶음콩,*,*,*,*,*\n",
    "상하목장,,,,NNG,*,T,상하목장,*,*,*,*,*\n",
    "요거트,,,,NNG,*,F,요거트,*,*,*,*,*\n",
    "카카오닙스,,,,NNG,*,F,카카오닙스,*,*,*,*,*\n",
    "잇클린,,,,NNG,*,T,잇클린,*,*,*,*,*\n",
    "카페다이어트,,,,NNG,*,F,카페다이어트,*,*,*,*,*\n",
    "사과맛워터젤리,,,,NNG,*,F,사과맛워터젤리,*,*,*,*,*\n",
    "맛있는다이어트칩,,,,NNG,*,T,맛있는다이어트칩,*,*,*,*,*\n",
    "다이어트칩,,,,NNG,*,T,다이어트칩,*,*,*,*,*'''.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, i in enumerate(keyword_dic):\n",
    "    keyword_dic[idx] = i.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(keyword_dic).to_csv('c:\\\\mecab\\\\user-dic\\\\intake_dic.csv', encoding='utf-8', index=None, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'_'.join(['sadf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\n",
    "for idx in range(len(intake.morph_list)):\n",
    "    print(intake.morph_list[idx])\n",
    "    print(intake.modified[idx, 2])\n",
    "    print(idx)\n",
    "    k = input()    \n",
    "    while k:\n",
    "        keyword_list.append(k)\n",
    "        k = input()\n",
    "        if k == 'break' or k == 'next':\n",
    "            break\n",
    "    if k == 'break':\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m이 False인 경우에는 진짜 맛있다는 것.\n",
    "## -> '없' 이라는 음절이 '있'이라는 음절 이후 4음절 내에 없다는 뜻이다.\n",
    "## m이 True인 경우에는 애매한 상황이다. '없' 이라는 음절이 4음절 내에 존재한다는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "m = re.search('맛.{0,5}있.{0,5}[없않]', '안녕하세요. 맛이 있진 않다.. 다만 아쉬운 것은 포장지가 없다.')\n",
    "print(m)\n",
    "print('맛있다는 뜻' if m == None else '맛 없다는 뜻\\n원문: ' + m.group())\n",
    "\n",
    "m = re.search('맛.{0,5}있.{0,5}[없않]', '안녕하세요. 맛이 있다. 다만 아쉬운 것은 포장지가 없다.')\n",
    "print(m)\n",
    "print('맛있다는 뜻' if m == None else '맛 없다는 뜻\\n원문: ' + m.group())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab.pos('모닝죽')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper(df, tag='NNG'):\n",
    "    for i in range(len(df)):\n",
    "        temp = mecab.pos(df.iloc[i,0])\n",
    "        df.iloc[i, 0] = df.iloc[i,0] + '_' + df.iloc[i, 1]\n",
    "        for idx, j in enumerate(temp):\n",
    "            temp[idx] = '_'.join(list(j))\n",
    "        df.iloc[i, 1] = '/'.join(temp)\n",
    "    df.to_csv('a.csv', encoding='cp949')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helper(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a = '''곤약현미밥_NNG_T_곤약현미밥\n",
    "버터치킨커리_NNG_F_버터치킨커리\n",
    "유지어터_NNG_F_유지어터\n",
    "치킨커리_NNG_F_치킨커리\n",
    "버터치킨_NNG_F_버터치킨\n",
    "귀리우유_NNG_F_귀리우유\n",
    "텐바이텐_NNG_T_텐바이텐\n",
    "텐텐쇼퍼_NNG_F_텐텐쇼퍼\n",
    "식사대용_NNG_T_식사대용*\n",
    "한끼_NNG_F_한끼\n",
    "밀스라이트_NNG_F_밀스라이트\n",
    "식단인증_NNG_F_식단인증\n",
    "직장인_NNG_T_직장인\n",
    "로얄밀크티_NNG_F_로얄밀크티\n",
    "곤약젤리_NNG_F_곤약젤리\n",
    "모닝채소_NNG_F_모닝채소\n",
    "콜드프레스_NNG_F_콜드프레스\n",
    "하루야채_NNG_F_하루야채\n",
    "씨리얼_NNG_T_씨리얼\n",
    "콜드부르_NNG_F_콜드부르\n",
    "건강스타그램_NNG_T_건강스타그램\n",
    "스타그램_NNG_T_스타그램\n",
    "모닝죽_NNG_T_모닝죽\n",
    "인테이크_NNG_F_인테이크\n",
    "꿀고구마_NNG_F_꿀고구마\n",
    "밀스칩_NNG_T_밀스칩\n",
    "미래지향_NNG_F_미래지향*미래\n",
    "고농축_NNG_T_고농축\n",
    "단백질_NNG_T_단백질\n",
    "밀스드링크딸기_NNG_F_밀스드링크딸기\n",
    "프레시모닝_NNG_T_프레시모닝\n",
    "대체_NNG_F_대체\n",
    "클랜즈_NNG_F_클랜즈\n",
    "얼리버드_NNG_F_얼리버드\n",
    "까페라떼_NNG_F_까페라떼\n",
    "스타벅스_NNG_F_스타벅스\n",
    "밀스드링크_NNG_F_밀스드링크\n",
    "존맛_VA_T_존맛\n",
    "아워홈_NNG_T_아워홈\n",
    "모닝바게트_NNG_F_모닝바게트\n",
    "모닝귀리_NNG_F_모닝귀리\n",
    "스윗그레인_NNG_T_스윗그레인\n",
    "그릭요거트_NNG_F_그릭요거트\n",
    "팥톡스_NNG_F_팥톡스\n",
    "베지밀_NNG_T_베지밀\n",
    "식이섬유_NNG_F_식이섬유\n",
    "식이조절_NNG_T_식이조절\n",
    "줌마_NNG_F_줌마\n",
    "매일유업_NNG_T_매일유업\n",
    "상하목장_NNG_T_상하목장\n",
    "요거트_NNG_F_요거트\n",
    "카카오닙스_NNG_F_카카오닙스\n",
    "잇클린_NNG_T_잇클린\n",
    "카페다이어트_NNG_F_카페다이어트\n",
    "사과맛워터젤리_NNG_F_사과맛워터젤리'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b= a.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b = [i.split('_') for i in b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b = pd.DataFrame(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['두_MM', '번_NNBC', '째_XSN', '샘플_NNG', '폭탄_NNG', '이뻐_VA+EC', '죽_VV', '겠_EP', '어_EF', '이거_NP', '먹_VV', '고_EC', '오빠_NNG', '가_JKS', '세계_NNG', '클래식_NNG', '선발전_NNG', '잘_MAG', '치루_VV', '올께_NNG', '사업_NNG', '의_JKG', '성공_NNG', '을_JKO', '기원_NNG', '하_XSV', '며_EC', '스테파노_NNP', '아멘_NNG', '외칩니다_VV+EF', '찍찍_MAG', '인테이크_NNG', '제품_NNG', '파워젤부스트_NNG', '식감_NNG', '이_JKS', '놀라웠_VA+EP', '으며_EC', '요새_NNG', '하루견과_NNG', '류_XSN', '는_JX', '견과류_NNG', '반_NNG', '말린_VV+ETM', '과일_NNG', '이_VCP', '라서_EC', '는_ETM', '거_NNB', '보다_JKB', '버리_VV', '게_NNB+JKS', '인_VCP+ETM', '반면_NNG', '닥터_NNG', '넛츠_NNP', '프리미엄_NNG', '골드_NNP', '통째_NNG', '로_JKB', '만_JX', '들_VV', '어_EC', '있_VX', '가성비_NNG', '갑_NNG', '내_NP+JKG', '스타일_NNG', '야_EC', '쌀_NNG', '싶_VX', '을_ETM', '땐_NNG+JX', '모닝죽_NNG', '단호박_NNG', '모닝죽고구마_NNG', '모닝죽단팥_NNG', '모닝죽귀리_NNG']\n",
      "momsterz_kr Repost @hyunsukstagram (@get_repost)\n",
      "・・・\n",
      "두번째 샘플폭탄 이뻐죽겠어  이거먹고 오빠가  세계클래식 선발전 잘 치루고 올께� 사업의 성공을 기원하며�  스테파노 가  아멘 을 외칩니다  찍찍�\n",
      "@muleah_ @leahmkim @momsterz_kr\n",
      " 인테이크 제품의  파워젤부스트 의 식감이 놀라웠으며��� 요새  하루견과 제품류는 견과류반 말린과일 반이라서 먹는거보다 버리는게 반인반면  닥터넛츠 의 프리미엄골드 견과류는 25g 통째로 견과류만 들어있는게 가성비 갑��  내스타일이야�  쌀이먹고싶을땐  모닝죽 모닝죽단호박  모닝죽고구마  모닝죽단팥  모닝죽귀리\n",
      "2000\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m    728\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 729\u001b[1;33m                 \u001b[0mident\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    730\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\jupyter_client\\session.py\u001b[0m in \u001b[0;36mrecv\u001b[1;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[0;32m    802\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 803\u001b[1;33m             \u001b[0mmsg_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    804\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\zmq\\sugar\\socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[1;34m(self, flags, copy, track)\u001b[0m\n\u001b[0;32m    394\u001b[0m         \"\"\"\n\u001b[1;32m--> 395\u001b[1;33m         \u001b[0mparts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    396\u001b[0m         \u001b[1;31m# have first part already, only loop while more to receive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-fcaf955021ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintake\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodified\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mkeyword_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m             \u001b[0mpassword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    705\u001b[0m         )\n\u001b[0;32m    706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m    732\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 734\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    735\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for idx in range(len(intake.morph_list)):\n",
    "    idx = idx + 2000\n",
    "    print(intake.morph_list[idx])\n",
    "    print(intake.modified[idx, 2])\n",
    "    print(idx)\n",
    "    k = input()    \n",
    "    while k:\n",
    "        keyword_list.append(k)\n",
    "        k = input()\n",
    "        if k == 'break' or k == 'next':\n",
    "            break\n",
    "    if k == 'break':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nan'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "str(np.nan)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
