{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abys\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from ckonlpy.tag import Twitter\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.corpora import Dictionary\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import mglearn\n",
    "from pprint import pprint\n",
    "from ckonlpy.tag import Twitter\n",
    "import numpy as np\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "class Social_analysis():\n",
    "    \n",
    "    non_bmp_map = dict.fromkeys(range(0x10000, sys.maxunicode + 1), 0xfffd)\n",
    "\n",
    "    def __init__(self):\n",
    "        self.twitter = Twitter()\n",
    "        \n",
    "    def pickle_to_table(self, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        data = data[1:]\n",
    "        for idx, i in enumerate(data):\n",
    "            data[idx][2] = i[2].replace('#', ' ').translate(self.non_bmp_map)\n",
    "            data[idx][3] = '/'.join(i[3])\n",
    "            data[idx][4] = '/'.join(i[4])\n",
    "        self.raw_data = np.array(data)\n",
    "  \n",
    "    def hashtags_split(self, hashtags):        \n",
    "        hashtags_split = []\n",
    "        for i in hashtags:\n",
    "            hashtags_split.append(i.split('/'))\n",
    "        \n",
    "        hashtags_list = []\n",
    "        \n",
    "        for i in hashtags_split:\n",
    "            temp = []\n",
    "            for j in i:\n",
    "                if self.isHangul(j):\n",
    "                    t_hashtags = j.translate(self.non_bmp_map)\n",
    "                    temp.append(t_hashtags)\n",
    "            hashtags_list.append(temp)\n",
    "        self.hashtags_list = hashtags_list\n",
    "        \n",
    "        return hashtags_list\n",
    "                    \n",
    "    def add_keyword_dic(self, keyword_list, tag='Noun'):\n",
    "        for i in keyword_list:\n",
    "            if type(i) == tuple:\n",
    "                self.twitter.add_dictionary(i[0], i[1])\n",
    "            else:\n",
    "                self.twitter.add_dictionary(i, tag)\n",
    "        \n",
    "    def morph_pos(self, text_list, exception_list = ['ë§›', 'ë°¥', 'ë¬¼', 'ëª¸']):\n",
    "        \n",
    "        morph_list = []\n",
    "        noun_list = []\n",
    "        adj_list = []\n",
    "        verb_list = []\n",
    "        \n",
    "        for j in text_list:\n",
    "            parsed = self.twitter.pos(j)\n",
    "            temp = []\n",
    "            n_temp = []\n",
    "            adj_temp = []\n",
    "            verb_temp = []\n",
    "            \n",
    "            for i in parsed:\n",
    "                if self.isHangul(i[0]):\n",
    "                    if ((len(i[0]) > 1) or (i[0] in exception_list)):\n",
    "                        temp.append(i)\n",
    "                        if i[1] == 'Noun':\n",
    "                            n_temp.append(i[0])\n",
    "                        elif i[1] == 'Verb':\n",
    "                            adj_temp.append(i[0])\n",
    "                        elif i[1] == 'Adjective':\n",
    "                            verb_temp.append(i[0])\n",
    "                    else:\n",
    "                        print('{} ì œì™¸'.format(i[0]))\n",
    "                else: print('{} í•œê¸€ì´ ì•„ë‹˜.'.format(i[0]))\n",
    "            \n",
    "\n",
    "            morph_list.append(temp)\n",
    "            noun_list.append(n_temp)\n",
    "            adj_list.append(adj_temp)\n",
    "            verb_list.append(verb_temp)\n",
    "\n",
    "        nav_list = noun_list + adj_list + verb_list\n",
    "\n",
    "        return morph_list, nav_list, noun_list, adj_list, verb_list\n",
    "\n",
    "\n",
    "    \n",
    "    def pos_extractor(self, parsed):\n",
    "        for i in parsed:            \n",
    "            temp = []\n",
    "            n_temp = []\n",
    "            adj_temp = []\n",
    "            verb_temp = []\n",
    "            if self.isHangul(i[0]):\n",
    "                if ((len(i[0]) > 1) or (i[0] in exception_list)):\n",
    "                    temp.append(i)\n",
    "                    if i[1] == 'Noun':\n",
    "                        n_temp.append(i[0])\n",
    "                    elif i[1] == 'Verb':\n",
    "                        adj_temp.append(i[0])\n",
    "                    elif i[1] == 'Adjective':\n",
    "                        verb_temp.append(i[0])\n",
    "                else:\n",
    "                    print('{} ì œì™¸'.format(i[0]))\n",
    "            else: print('{} í•œê¸€ì´ ì•„ë‹˜.'.format(i[0]))\n",
    "            \n",
    "\n",
    "            morph_list.append(temp)\n",
    "            noun_list.append(n_temp)\n",
    "            adj_list.append(adj_temp)\n",
    "            verb_list.append(verb_temp)\n",
    "\n",
    "        nav_list = noun_list + adj_list + verb_list\n",
    "\n",
    "        return morph_list, nav_list, noun_list, adj_list, verb_list\n",
    "\n",
    "    \n",
    "    \n",
    "    def merge_list(self, tokenized_list):\n",
    "        return [j for i in tokenized_list for j in i]\n",
    "\n",
    "    \n",
    "    def join_list(self, tokenized_list):\n",
    "        joined_list = []\n",
    "        for idx, i in enumerate(tokenized_list):\n",
    "            joined_list.append(\" \".join(i))\n",
    "        return joined_list\n",
    " \n",
    "    def split_list(self, untokenized_list):\n",
    "        hashtag_splited = []\n",
    "        for idx, i in enumerate(untokenized):\n",
    "            hashtag_splited.append(i.split('/'))\n",
    "            return hastag_splited\n",
    "\n",
    "    def word_substitute(self, dataset, sublist):\n",
    "        dataset = copy.deepcopy(dataset)\n",
    "        sub_book = dict()\n",
    "        for i in sublist:\n",
    "            for j in i['sub_words']:\n",
    "                sub_book[j] = i['main']\n",
    "        gc.collect()\n",
    "        for n, i in enumerate(dataset):\n",
    "            dataset[n] = [sub_book.get(item,item) for item in i]\n",
    "\n",
    "        del sub_book\n",
    "        gc.collect()\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    def word_delete(self, dataset, del_list):\n",
    "        dataset = copy.deepcopy(dataset)\n",
    "\n",
    "        for n, line in enumerate(dataset):\n",
    "             dataset[n] = [i for i in line if i not in del_list]\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    \n",
    "    def isHangul(self, text):\n",
    "        encText = text\n",
    "        hanCount = len(re.findall(u'[\\u3130-\\u318F\\uAC00-\\uD7A3]+', encText))\n",
    "        return hanCount > 0\n",
    "\n",
    "    \n",
    "\n",
    "class SB_Word2Vec():    \n",
    "    \n",
    "    def __init__(self, morph_list):\n",
    "        self.dct = Dictionary(morph_list)\n",
    "        self.corpus = [self.dct.doc2bow(line) for line in morph_list]\n",
    "        self.build_Word2Vec(morph_list)\n",
    "    \n",
    "    def make_Word2Vec(self, morph_list, size=50, window=2, min_count=10, iteration=100):\n",
    "        self.em = Word2Vec(morph_list, size=size, window=window, min_count=min_count, iter=iteration)\n",
    "        self.em_vocab = list(self.em.wv.vocab.keys())\n",
    "        self.em_vocab_dic = {word:idx for idx, word in enumerate(self.em_vocab)}\n",
    "\n",
    "    def make_Word2Sen_matrix(self): \n",
    "        vocab_size = len(self.em_vocab)\n",
    "        self.sen_matrix = np.zeros((len(self.corpus), vocab_size))\n",
    "        for idx, row in enumerate(self.sen_matrix):\n",
    "            for idx2, frequency in self.corpus[idx]:\n",
    "                    if self.dct[idx2] in self.em_vocab:\n",
    "                        self.sen_matrix[idx][self.em_vocab_dic[self.dct[idx2]]] = frequency                \n",
    "        self.sim_matrix = np.zeros((vocab_size, vocab_size))\n",
    "        for idx, w1 in enumerate(self.em_vocab):\n",
    "            for idx2, w2 in enumerate(self.em_vocab):\n",
    "                self.sim_matrix[idx][idx2] =  self.em.wv.similarity(w1, w2)\n",
    "\n",
    "        self.word2sen_matrix = np.dot(self.sim_matrix, np.transpose(self.sen_matrix))\n",
    "\n",
    "        return self.word2sen_matrix\n",
    "\n",
    "    def get_sim_sen(self, keyword, main_text, number=1):\n",
    "        self.sim_sen_index = np.argsort(self.word2sen_matrix[self.em_vocab_dic[keyword]])\n",
    "        self.most_sim_sen_index = np.argmax(self.word2sen_matrix[self.em_vocab_dic[keyword]])\n",
    "        index_list = self.sim_sen_index.reshape((-1,)).tolist()\n",
    "        index_list.reverse()\n",
    "        \n",
    "        for idx, i in enumerate(index_list[:number]):\n",
    "            print(str(idx + 1))\n",
    "            print(main_text[i])\n",
    "        return index_list\n",
    "    \n",
    "    def build_Word2Vec(self, morph_list):\n",
    "        self.make_Word2Vec(morph_list)\n",
    "        self.make_Word2Sen_matrix()\n",
    "        \n",
    "        \n",
    "class SB_LDA():\n",
    "\n",
    "    def make_lda(self, morph_joined, ntopic=10, learning_method='batch', max_iter=25, random_state=0, n_words=20):        \n",
    "        self.vect = CountVectorizer(max_features=10000, max_df=.15)\n",
    "        self.X = self.vect.fit_transform(morph_joined)\n",
    "        self.lda = LatentDirichletAllocation(n_components=ntopic, learning_method=learning_method, max_iter=max_iter, random_state=random_state)\n",
    "        self.document_topics = self.lda.fit_transform(self.X)\n",
    "        self.sorting = np.argsort(self.lda.components_, axis=1)[:, ::-1]\n",
    "        self.feature_names = np.array(self.vect.get_feature_names())\n",
    "        mglearn.tools.print_topics(topics=range(ntopic), feature_names=self.feature_names, sorting=self.sorting, topics_per_chunk=5, n_words=n_words)\n",
    "\n",
    "    def related_doc(self, main_text_list, topic_index, number=10):\n",
    "        category = np.argsort(self.document_topics[:, topic_index])[::-1]\n",
    "        related_docs = []\n",
    "        for i in category[:number]:\n",
    "            print(i)\n",
    "            print(main_text_list[i] + \".\\n\")\n",
    "            related_docs.append((i, main_text_list[i]))\n",
    "        return related_docs\n",
    "\n",
    "class SB_Tfidf():    \n",
    "    \n",
    "    def __init__(self, list_morph_merged):\n",
    "        self.list_morph_merged = list_morph_merged\n",
    "        self.dct = Dictionary(self.list_morph_merged)\n",
    "        self.corpus = [self.dct.doc2bow(line) for line in self.list_morph_merged]\n",
    "\n",
    "    def get_tfidf(self):       \n",
    "        self.model = TfidfModel(self.corpus)\n",
    "        self.tfidf = []\n",
    "        for i in self.corpus:\n",
    "             self.tfidf.append(sorted(self.model[i], key = lambda x: x[1], reverse=True))\n",
    "        self.tfidf_hangul = []\n",
    "        for idx1, i in enumerate(self.tfidf):\n",
    "            self.tfidf_hangul.append([(self.dct[j[0]], j[1]) for j in i])        \n",
    "        \n",
    "        return self.tfidf_hangul\n",
    "    \n",
    "def frequency(merged):\n",
    "    word_count = Counter(merged)\n",
    "    word_count2 = []\n",
    "    for i in word_count:\n",
    "        word_count2.append((i, word_count[i]))\n",
    "    word_count2 = sorted(word_count2, key=lambda x: x[1], reverse = True)\n",
    "    return word_count2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake = Social_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labnosh = Social_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBì—ì„œ ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ë¥¼ Social_analysis ì¸ìŠ¤í„´ìŠ¤ì— ë“±ë¡í•´ë„  ë˜ê³ ,  ê·¸ëƒ¥ ê·¸ ë°ì´í„°ë¡œ ì•ìœ¼ë¡œ ì§„í–‰í•´ë„ ëœë‹¤. ì¼ë‹¨ íŠœí† ë¦¬ì–¼ì€ pickle ë°ì´í„°ë¡œ ì§„í–‰í•˜ê¸° ë•Œë¬¸ì— ì•„ë˜ í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ë©´ intake.row_data ì— ì €ì¥ì´ ëœë‹¤. ì´ ë†ˆì€ DBì— ë“¤ì–´ê°„ ë†ˆì´ ë¹„ìŠ·í•˜ê²Œ ìƒê¸´ ë†ˆì´ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake.pickle_to_table('Data/intake_list.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labnosh.pickle_to_table('Data/labnosh_list.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í˜•íƒœì†Œ ë¶„ì„ì— ê¼­ í¬í•¨í•˜ê³  ì‹¶ì€ ë‹¨ì–´ë¥¼ ì¶”ê°€í•œë‹¤. ê¸°ë³¸ì ìœ¼ë¡œëŠ” hashtagë“¤ì„ nounìœ¼ë¡œ ì¶”ê°€í•œë‹¤. ì—¬ê¸°ì„œ í•´ì‰¬íƒœê·¸ ì¤‘ì— í¬í•¨í•˜ê³  ì‹¶ì§€ ì•Šì€ ê²ƒì„ ë¹¼ëŠ” ê²ƒì„ ì¶”ì²œí•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5235,)\n",
      "['ì•„ì¹¨ì‹ì‚¬', 'ì£½', 'ëª¨ë‹ì£½', 'ë‹¨í˜¸ë°•', 'ì¸í…Œì´í¬', 'ë§›ìŠ¤íƒ€ê·¸ë¨', 'ë¨¹ìŠ¤íƒ€ê·¸ë¨', 'ì‹ì‚¬ëŒ€ìš©', 'ë‹¤ì´ì–´íŠ¸', 'ë‘í†µ', 'ì¢‹ì•„ìš”ë°˜ì‚¬', 'ë§›ì§‘', 'ê°„í¸ì‹', 'ì•„ì ', 'ì•„í””', 'ì„ íŒ”ì€ë§íŒ”', 'ë§›ìŠ¤íƒ€', 'ì„ íŒ”', 'ë§íŒ”', 'ë¨¹ìŠ¤íƒ€', 'ìŒì‹', 'í‘¸ë“œìŠ¤íƒ€ê·¸ë¨', 'ì¢‹ë°˜']\n"
     ]
    }
   ],
   "source": [
    "intake.hashtag_splited = intake.hashtags_split(intake.raw_data[:, 3])\n",
    "print(np.shape(intake.hashtag_splited))\n",
    "print(intake.hashtag_splited[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7967,)\n",
      "['ì™€ì´ì‹ë‹¨', 'ì™€ì´_ë°›ì•˜ë‹¤ê·¸ë¨', 'ì—ì‡í…œ', 'í‘¸ë“œë°”', 'í‘¸ë“œë°”', 'í”„ë¡œí‹´ë°”', 'í‘¸ë“œë°”', 'í‘¸ë“œë°”', 'ì—ì‡í…œ', 'ë©ë…¸ì‰¬', 'ì´ˆì½”ë°”', 'í”„ë¡œí‹´', 'ë‹¤ì´ì–´íŠ¸', 'ì‹ë‹¨', 'ë‹¤ì´ì–´íŠ¸ì‹ë‹¨', 'ì‹ë‹¨ê´€ë¦¬', 'ê±´ê°•ì‹', 'ë‹¤ì´ì–´íŠ¸ê·¸ë¨', 'ë‹¤ì´ì–´í„°ê·¸ë¨', 'ë‹¤ì´ì–´íŠ¸ê³„ì •', 'ë‹¤ì´ì–´íŠ¸ì†Œí†µ', 'ë‹¤ì´ì–´íŠ¸ê¸°ë¡']\n"
     ]
    }
   ],
   "source": [
    "labnosh.hashtag_splited = labnosh.hashtags_split(labnosh.raw_data[:, 3])\n",
    "print(np.shape(labnosh.hashtag_splited))\n",
    "print(labnosh.hashtag_splited[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['user_id', '2018-05-03T23:42:16.000Z',\n",
       "        'y_diet__ 20180504 ì˜¤ëŠ˜ì˜ ì•„ì¹¨ï¿½\\n ì™€ì´ì‹ë‹¨  ì™€ì´_ë°›ì•˜ë‹¤ê·¸ë¨\\nâœ”ï¸ ì—ì‡í…œ  í‘¸ë“œë°”\\nâœ”ï¸ìš°ìœ  í•œ ì”\\nì—ì‡í…œ ì´ë²¤íŠ¸ë¡œ ë°›ê²Œ ëœ ì‹ ì œí’ˆâ€¼ï¸  í‘¸ë“œë°” ï¿½ï¿½\\nì¼ë‹¨ ì´í‰ì€! ìš”ì¦˜ ë§ë¡œ \"ê°“ ëµ í…œ ï¿½ï¿½\\u200dâ™€ï¸\" ì¼ë‹¨ ë‹¤ì´ì–´íŠ¸í•˜ë©´ì„œ ì´ˆì½” ì¢‹ì•„í•˜ì‹œëŠ”ë¶„ë“¤ì´ë©´ ë‹¹ì¥ ì‚¬ì£¼ì„¸ìš”! ë¸Œë¼ìš°ë‹ˆ ë¨¹ëŠ” ëŠë‚Œ, ì•„ë‹ˆë©´ ì‹œì¤‘ì— íŒŒëŠ” ì´ˆì½”ë°” ë¨¹ëŠ” ëŠë‚Œ! ë„ˆë¬´ ê¾¸ë•, ë‹¨ë‹¨í•˜ì§€ë„ ì•Šìœ¼ë©´ì„œ ë§‰ ì´‰ì´‰í•˜ì§€ë„ ì•Šì€ ì¤‘ê°„ ì •ë„ì˜ ì‹ê°ì— ì¤‘ê°„ ì¤‘ê°„ ì´ˆì½œë¦¿ì´ ë°•í˜€ìˆê³ , ì½”ì½”ë„›ê¹Œì§€ ë“¤ì–´ìˆì–´ì„œ ì‹ê° ìµœê³ !ï¿½ï¿½\\n95% ì´ìƒì´ ìœ ê¸°ë† ì›ë£Œë¡œ ë§Œë“¤ì–´ì¡Œê¸° ë•Œë¬¸ì— ë¯¿ê³  ì•ˆì‹¬í•˜ê³  ë¨¹ì„ ìˆ˜ ìˆê³ , 200kcalì— ì‹ì´ì„¬ìœ ê°€11g (44%), ë‹¨ë°±ì§ˆ 13g(24%) ë“± ì˜ì–‘ì„±ë¶„ë„ ìµœê³ ..!ï¿½ï¿½\\në‹¹ë¥˜ëŠ” 9g(9%) ì •ë„ì¸ë°, ê°œì¸ì ìœ¼ë¡œ ë‹¨ ê±¸ ì¢‹ì•„í•˜ëŠ” í¸ì´ ì•„ë‹ˆë¼ì„œ í‘¸ë“œë°”ë¥¼ ë‹¤ ë¨¹ì–´ê°ˆë•Œì¯¤ ë‹¬ì•„ì„œ ì¡°ê¸ˆ ë¬¼ë¦°ë‹¤(?)ëŠ” ëŠë‚Œì„ ë°›ì•˜ì§€ë§Œ! ê·¸ëŸ´ ë• ìš°ìœ  í•œ ëª¨ê¸ˆ ë§ˆì…”ì£¼ë©´ ë‹¤ì‹œ í™€ë¦°ë“¯ì´ ì°¾ê²Œ ë˜ëŠ” ë§ˆì„±ì˜ ë§¤ë ¥ì„ ê°€ì§„ í‘¸ë“œë°”âœ¨\\në˜ë‹¤ë¥¸ ì‹ ì œí’ˆ í‘¸ë“œì‰ì´í¬ë„ ê°™ì´ ë³´ë‚´ì£¼ì…¨ê¸¸ë˜ ê°ë™ë°›ì•„ì„œ ì§€ê¸°ë‹˜ê»˜ ë””ì—  ë³´ëƒˆë”ë‹ˆ ë„˜ ìŠ¤ìœ—í•œ ë‹µì¥ ë³´ë‚´ì£¼ì…”ì„œ ì‹¬ì¿µâ¤ï¸\\nì•„ë¬´íŠ¼ ì´ê±° ì§„ì§œ ìµœê³   í”„ë¡œí‹´ë°”  í‘¸ë“œë°” â€¼ï¸ ë‹¤ ë¨¹ìœ¼ë©´ ë¬´ì¡°ê±´ ì¬êµ¬ë§¤ í• ê±°ì—ìš”!! ì´ˆì½” ì¢‹ì•„í•˜ì‹œëŠ” ë‹¤ì´ì–´í„°ë¼ë©´ ê¼­ ë“œì…”ë³´ì„¸ìš” ! í›„íšŒí•˜ì§€ ì•Šìœ¼ì‹¤ê±°ì—ìš”ï¿½\\nê°„ë‹¨í•œ í•œ ë¼ ì‹ì‚¬ë¡œ, í˜¹ì€ ì˜¤í›„ì— ë‹¹ ë–¨ì–´ì§ˆ ë•Œ ê°„ì‹ìœ¼ë¡œ ë¨¹ìœ¼ë©´ ë”± ì¢‹ì„ ì•„ì´í…œì…ë‹ˆë‹¤! ì´ëŸ° ì¢‹ì€ ì œí’ˆ ë§Œë“¤ì–´ ì¤€ ì—ì‡í…œì—ê²Œ ê°ì‚¬ë¥¼!!ï¿½ï¿½â¤ï¸',\n",
       "        '20180504/ì™€ì´ì‹ë‹¨/ì™€ì´_ë°›ì•˜ë‹¤ê·¸ë¨/ì—ì‡í…œ/í‘¸ë“œë°”/í‘¸ë“œë°”/í”„ë¡œí‹´ë°”/í‘¸ë“œë°”/í‘¸ë“œë°”/ì—ì‡í…œ/ë©ë…¸ì‰¬/ì´ˆì½”ë°”/í”„ë¡œí‹´/ë‹¤ì´ì–´íŠ¸/ì‹ë‹¨/ë‹¤ì´ì–´íŠ¸ì‹ë‹¨/ì‹ë‹¨ê´€ë¦¬/foodbar/diet/eatclean/ê±´ê°•ì‹/ë‹¤ì´ì–´íŠ¸ê·¸ë¨/ë‹¤ì´ì–´í„°ê·¸ë¨/ë‹¤ì´ì–´íŠ¸ê³„ì •/ë‹¤ì´ì–´íŠ¸ì†Œí†µ/ë‹¤ì´ì–´íŠ¸ê¸°ë¡/bbin_d/young.luvv/im_sooki/juning_diet/__diet_bin/h.jin_0103/nari2323',\n",
       "        'y_diet__#í‘¸ë“œë°” #ì—ì‡í…œ #ë©ë…¸ì‰¬ #ì´ˆì½”ë°” #í”„ë¡œí‹´ #ë‹¤ì´ì–´íŠ¸ #ì‹ë‹¨ #ë‹¤ì´ì–´íŠ¸ì‹ë‹¨ #ì‹ë‹¨ê´€ë¦¬ #foodbar #diet #eatclean #ê±´ê°•ì‹ #ë‹¤ì´ì–´íŠ¸ê·¸ë¨ #ë‹¤ì´ì–´í„°ê·¸ë¨ #ë‹¤ì´ì–´íŠ¸ê³„ì • #ë‹¤ì´ì–´íŠ¸ì†Œí†µ #ë‹¤ì´ì–´íŠ¸ê¸°ë¡/oklmbbiní‘¸ë“œë°” ê¶ê¸ˆí•˜ë„¤ì˜¤ ì«€ë“í•´ë³´ì´ë‰¸.../tjdworb5ì˜¤ëŠ˜ë„ ì¢‹ì€í•˜ë£¨!!/young.luvví—ë­ë°©êµ¬ ì´ê±°ìŠ¨ ì €ë¥¼ ìœ„í•œê±´ê°€ìš”/im_sookiìš”ë†ˆë³´ê²Œ.. ì—„ì²­ë‚œ ê¾¸ë•ë‹¬ë‹¬ê±´ê°•ë§Œì ì´ ì¼ê²ƒ ê°™ì•„ì—¬!!!â™¥ ì—ì‡í…œ êµ¬ê²½ê°€ë´ì•¼ê² ì–´ìš”! ã…‹ã…‹ã…‹ã…‹ã…‹/y_diet__@bbin_d ì§„ì§œ ìµœê³ ì—ìš”ã… ã…  ë§‰ ì«€ë“í•´ì„œ ì´ì— ë‹¬ë¼ë¶™ëŠ” ê²ƒë„ ì•„ë‹ˆêµ¬ ì ë‹¹íˆ ì¬°ë“ì´‰ì´‰..â™¥/y_diet__@young.luvv ì˜ë‹˜ ã…  êµ¬ë§¤ê°ì´ì—ì—¬!!!! ì €ë„ ë‹¤ë¨¹ê³  êµ¬ë§¤í•˜ë ¤êµ¬ìš˜ğŸ‘€â¤ï¸/y_diet__@im_sooki ìˆ˜í‚¤ë‹˜ ì •í™•í•´ì˜„ã…‹ã…‹ã…‹ã…‹ã…‹ê¾¸ë•í•˜ê³  ë‹¬ë‹¬í•˜ë©´ì„œ ê±´ê°•ë§Œì ! ã…  ì´ëŸ°ê±° ë¨¹ìœ¼ë©´ì„œ ë‹¤ì´ì–´íŠ¸í•´ë„ ë˜ë‚˜ ì‹¶ì„ì •ë„ì˜ ë§›ì´ì—ìš”ğŸ‘ğŸ»â¤ï¸/juning_dietê·¸ëƒ¥ ë³´ê¸°ë§Œí•´ë‘ ë“ ë“ í• ê²ƒ ê°™ì€ í‘¸ë“œë°”ì´ë„¤ìš” ğŸ’šğŸ’šğŸ’š/__diet_binì™€ ë¹„ì£¼ì–¼ë¶€í„°ê°€ ì°ë“í•˜ë‹ˆ ìš´ë™ ì „ ì— ê°„ì‹ëŒ€ìš©ìœ¼ë¡œ ì§„ì§œ ìµœê³ ì¼êº¼ ê°™ì•„ì—¬ !!! êº„/h.jin_0103ì™„ì „..ê¾¸ë•...?!...ê°€ê²©ì´..ğŸ˜³ğŸ˜‚â‰ï¸/y_diet__@juning_diet ì§„ì§œ ì´ˆì½”ì¢‹ì•„í•˜ì‹œëŠ”ë¶„ë“¤ì€ ê¼­ ì‚¬ì•¼ë©ë‹ˆë‹¹ ã… ã… \\U0001f92dâ™¥ï¸/y_diet__@__diet_bin ë¹„ë‹ˆë‹˜ ì§„ì¨” ìµœê³ ì—ìš˜.. ì´ˆì½”ì´ˆì½” ê¾¸ë•ì¬°ë“ì´‰ì´‰..â™¥/y_diet__@h.jin_0103 10ê°œì…ì— 24,000ì›ì´ì—ìš”!!/lee_suyeon91@nari2323 ëŒ€ë°•..',\n",
       "        None,\n",
       "        'https://www.instagram.com/p/BiVYjk4AWhX/?hl=ko&tagged=%EB%9E%A9%EB%85%B8%EC%89%AC'],\n",
       "       ['user_id', '2018-07-03T22:17:08.000Z',\n",
       "        'csmansoo2075ë¹µìˆœì´ ë–¡ìˆœì´ ë°€ê°€ë£¨ì¤‘ë…ì êµ°ê²ƒì§ˆë•í›„ì¸ ì €ì—ê²Œ ê±´ê°•í•œ ê°„ì‹ì´ ë„ì°©í–ˆì–´ìš”^^\\nê·¸ë•Œê·¸ë•Œ ê³¨ë¼ ë¨¹ì„ ìˆ˜ ìˆëŠ” ë‹¤ì„¯ê°€ì§€ ë§› í‘¸ë“œì‰ì´í¬ ìŠ¤íƒ€í„°í‚¤íŠ¸ì™€ ê¾¸ë•ê¾¸ë• ë„ˆë¬´ ë§›ìˆëŠ” ë‹¨ë°±ì§ˆ ê°€ë“ í‘¸ë“œë°”!\\ní‘¸ë“œì‰ì´í¬ëŠ” í”Œë¼ìŠ¤í‹± ìš©ê¸°ë¡œ ë˜ì–´ ìˆì–´ ë¬¼ì´ë‚˜ ìš°ìœ  ë¶€ì–´ì„œ ë¨¹ê¸° ë„ˆë¬´ í¸í•˜ê³  í‘¸ë“œë°”ëŠ” ì´ˆì½”ì´ˆì½” ë§›ë„ ê·¸ë ‡ê³  ê¾¸ë•ê¾¸ë• ê±´ê°•í•œ ë¸Œë¼ìš°ë‹ˆ ë¨¹ëŠ”ê²ƒ ê°™ì€ ëŠë‚Œì´ë„¤ìš”\\nì œê°€ ë„ˆë¬´ ì¢‹ì•„í•˜ëŠ” ë§›ì´ì—ìš”^^\\níœ´ëŒ€ê°€ ê°„í¸í•œ í‘¸ë“œë°”ë‘ í‘¸ë“œì‰ì´í¬ í•˜ë‚˜ì”© ê°€ë°©ì— ì˜ì˜¥~ ì±™ê²¨ ë‚˜ê°€ì„œ ì… ì‹¬ì‹¬í•  ë•Œ ê±´ê°•í•œ ê°„ì‹ì´ë‚˜ í•œë¼ ì‹ì‚¬ë¡œ ë¨¹ìœ¼ë ¤êµ¬ìš”^^\\nì¸ì¹œë‹˜ë“¤ë„ ê±´ê°•í•œí•˜ë£¨ ë³´ë‚´ì„¸ìš”ï¿½ï¿½\\n.\\n.\\n.\\nï¿½ ë©ë…¸ì‰¬ @labnosh.official\\n ìŠ¤íƒ€í„°í‚¤íŠ¸ í‘¸ë“œì‰ì´í¬ ë‹¨ë°±ì§ˆì‰ì´í¬ ì„ ì‹ í‘¸ë“œë°” ë‹¨ë°±ì§ˆë°” ì•„ì¹¨ë°¥ ë‹¤ì´ì–´íŠ¸ ë‹¤ì´ì–´íŠ¸ì‹ë‹¨ ë‹¤ì´ì–´íŠ¸ìš”ë¦¬ í´ë¦°í‘¸ë“œ ë¸ŒëŸ°ì¹˜ ì§‘ë°¥ í˜¼ë°¥ í™ˆì¿¡ í™ˆë©”ì´ë“œ ì˜¨ë”í…Œì´ë¸” í”Œë ˆì´íŒ… í‚¨í¬í¬ ë§›ìŠ¤íƒ€ê·¸ë¨ ë¨¹ìŠ¤íƒ€ê·¸ë¨ ìš”ë¦¬ìŠ¤íƒ€ê·¸ë¨ ì§‘ë°¥ìŠ¤íƒ€ê·¸ë¨ onthetable eatclean cleanfood instafood foodstagram',\n",
       "        'ë©ë…¸ì‰¬/labnosh.official/ìŠ¤íƒ€í„°í‚¤íŠ¸/í‘¸ë“œì‰ì´í¬/ë‹¨ë°±ì§ˆì‰ì´í¬/ì„ ì‹/í‘¸ë“œë°”/ë‹¨ë°±ì§ˆë°”/ì•„ì¹¨ë°¥/ë‹¤ì´ì–´íŠ¸/ë‹¤ì´ì–´íŠ¸ì‹ë‹¨/ë‹¤ì´ì–´íŠ¸ìš”ë¦¬/í´ë¦°í‘¸ë“œ/ë¸ŒëŸ°ì¹˜/ì§‘ë°¥/í˜¼ë°¥/í™ˆì¿¡/í™ˆë©”ì´ë“œ/ì˜¨ë”í…Œì´ë¸”/í”Œë ˆì´íŒ…/í‚¨í¬í¬/ë§›ìŠ¤íƒ€ê·¸ë¨/ë¨¹ìŠ¤íƒ€ê·¸ë¨/ìš”ë¦¬ìŠ¤íƒ€ê·¸ë¨/ì§‘ë°¥ìŠ¤íƒ€ê·¸ë¨/onthetable/eatclean/cleanfood/instafood/foodstagram',\n",
       "        '', None,\n",
       "        'https://www.instagram.com/p/BkyTTcEgfBZ/?hl=ko&tagged=%EB%9E%A9%EB%85%B8%EC%89%AC']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labnosh.raw_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## intake.add_dic_keyword() í•¨ìˆ˜ëŠ” 1ì°¨ì› listë¥¼ ë°›ëŠ”ë‹¤.\n",
    "## ë°ì´í„°ì˜ í˜•íƒœëŠ” ['ë‹¨ì–´', 'ë‹¨ì–´', 'ë‹¨ì–´'] or [('ë‹¨ì–´', Tag), ('ë‹¨ì–´', Tag)]\n",
    "## TagëŠ” 'Noun', 'Verb', 'Adjective' ë“±ì´ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake.hashtag_merged = intake.merge_list(intake.hashtag_splited)\n",
    "\n",
    "intake.add_keyword_dic(intake.hashtag_splited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labnosh.hashtag_merged = labnosh.merge_list(labnosh.hashtag_splited)\n",
    "\n",
    "labnosh.add_keyword_dic(labnosh.hashtag_splited)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í˜•íƒœì†Œ ë¶„ì„ì„ ì§„í–‰í•œë‹¤. í•œê¸€ìëŠ” ì œì™¸ì‹œí‚¤ëŠ”ë° í˜¹ì‹œ í•„ìš”í•˜ë‹¤ê³  íŒë‹¨ë˜ëŠ” ê¸€ìëŠ” exception_list íŒŒë¼ë¯¸í„°ì— ë¦¬ìŠ¤íŠ¸ë¡œ ë„£ì–´ì£¼ë©´ ëœë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake.morph_list, intake.nav_list, intake.noun_list, intake.adj_list, intake.verb_list= intake.morph_pos(intake.raw_data[:, 2], exception_list=['ë§›', 'ë°¥', 'ë¬¼', 'ëª¸', 'ì£½'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labnosh.morph_list, labnosh.nav_list, labnosh.noun_list, labnosh.adj_list, labnosh.verb_list= labnosh.morph_pos(labnosh.raw_data[:, 2], exception_list=['ë§›', 'ë°¥', 'ë¬¼', 'ëª¸', 'ì£½'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë¹ˆë„ë¶„ì„ì—ëŠ” mergeëœ listê°€ í•„ìš”í•˜ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('í…Œì´í¬', 6618), ('ë‹¤ì´ì–´íŠ¸', 4639), ('ê·¸ë¨', 4087), ('ìŠ¤íƒ€', 2790), ('ì£½', 2511)]\n"
     ]
    }
   ],
   "source": [
    "intake.nav_merged = intake.merge_list(intake.nav_list)\n",
    "\n",
    "intake.nav_frequency = frequency(intake.nav_merged)\n",
    "\n",
    "print(intake.nav_frequency [:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ë©ë…¸ì‰¬', 10886), ('ë‹¤ì´ì–´íŠ¸', 2730), ('ë§›', 1694), ('ë¯¸ë˜í˜•ì‹ì‚¬', 1628), ('ì˜¬ë¦¬ë¸Œì˜', 1355)]\n"
     ]
    }
   ],
   "source": [
    "labnosh.nav_merged = labnosh.merge_list(labnosh.nav_list)\n",
    "\n",
    "labnosh.nav_frequency = frequency(labnosh.nav_merged)\n",
    "\n",
    "print(labnosh.nav_frequency [:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì—¬ê¸°ì„œ ë‚´ê°€ í˜•íƒœì†Œë¶„ì„ì‹œì— ì•ˆì§¤ë¦¬ê²Œ í•  ë‹¨ì–´ì™€ ì œì™¸í•˜ê³  ì‹¶ì€ ë‹¨ì–´, ëŒ€ì²´í•˜ê³  ì‹¶ì€ ë‹¨ì–´ë¥¼ ì„ íƒí•œë‹¤.\n",
    "## ì•ˆì§¤ë¦¬ê²Œ í•  ë‹¨ì–´ëŠ” ë‹¤ì‹œ ìœ„ë¡œ ëŒì•„ê°€ ì¶”ê°€í•œë‹¤.\n",
    "## ëŒ€ì²´í•˜ê³  ì‹¶ì€ ë‹¨ì–´ì™€ ì œì™¸í•˜ê³  ì‹¶ì€ ë‹¨ì–´ëŠ” ì•„ë˜ ë°©ë²•ìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤. ì¼ë‹¨ ìƒëµí•œë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë¶„ì„ì—ì„œ ì œì™¸í•  ë‹¨ì–´ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë„£ì–´ì¤€ë‹¤. ì¸ìë¡œ ë„£ì–´ì£¼ëŠ” ê°’ì€ ë³€í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë„£ì–´ë³´ê³  ë§ˆìŒì— ì•ˆë“¤ë©´ ë‹¤ì‹œ intake.nav_list ë¡œ ì ‘ê·¼í•˜ë©´ ì‚­ì œ ì „ íŒŒì¼ë¡œ ëŒì•„ê°ˆ ìˆ˜ ìˆë‹¤.\n",
    "## ì§„ì§œ í›„íšŒì—†ì„ ê²ƒ ê°™ì„ ë•Œ, intake.nav_listì— ì—…ë°ì´íŠ¸ í•˜ë„ë¡ í•˜ì."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "intake.nav_frequency = frequency(intake.nav_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ì¸í…Œì´í¬', 5727), ('ë‹¤ì´ì–´íŠ¸', 2167), ('ë°€ìŠ¤', 1490), ('ì•„ì¹¨', 1273), ('ëª¨ë‹ì£½', 1212)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intake.nav_frequency [:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ì•„ì¹¨ì‹ì‚¬', 'ì£½', 'ëª¨ë‹ì£½', 'ë‹¨í˜¸ë°•', 'ì¸í…Œì´í¬', 'ëª¨ë‹ì£½', 'ì•„ì¹¨', 'ê¸°ë„', 'ì…ë§›', 'í•˜ë‚˜', 'ì•„ì¹¨', 'ëšë”±', 'ë§›ì€', 'ì¢…ë¥˜', 'ë§›ìˆëŠ”ë°', 'ë‹¨í˜¸ë°•ì´', 'ê¿€ê³ êµ¬ë§ˆ']\n"
     ]
    }
   ],
   "source": [
    "modified_nav_list = intake.word_delete(intake.nav_list, ['ìš”ê±°', 'ì—¬ëŸ¬', 'êµ³ì´'])\n",
    "print(modified_nav_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì´ë²ˆì—ëŠ” ë‹¨ì–´ë¥¼ ëŒ€ì²´í•˜ëŠ” ê²ƒ, ë¦¬ìŠ¤íŠ¸ ì•ˆì— ë”•ì…”ë„ˆë¦¬ í˜•ì‹\n",
    "#### [{'main': 'ìµœì¢…ì ìœ¼ë¡œ ëŒ€ì²´í•˜ê³  ì‹¶ì€ ë‹¨ì–´', 'sub_words':['ëŒ€ì²´í•  ë‹¨ì–´', 'ëŒ€ì²´í•  ë‹¨ì–´']},\n",
    "####  {'main': 'ìµœì¢…ì ìœ¼ë¡œ ëŒ€ì²´í•˜ê³  ì‹¶ì€ ë‹¨ì–´', 'sub_words':['ëŒ€ì²´í•  ë‹¨ì–´', 'ëŒ€ì²´í•  ë‹¨ì–´']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ì•„ì¹¨ì‹ì‚¬', 'ì£½', 'ëª¨ë‹ì£½', 'ëª¨ë‹ì£½', 'ì¸í…Œì´í¬', 'ëª¨ë‹ì£½', 'ì•„ì¹¨', 'ê¸°ë„', 'ì…ë§›', 'í•˜ë‚˜', 'ì•„ì¹¨', 'ëšë”±', 'ë§›ì€', 'ì¢…ë¥˜', 'ë§›ìˆëŠ”ë°', 'ëª¨ë‹ì£½', 'ëª¨ë‹ì£½']\n"
     ]
    }
   ],
   "source": [
    "modified_nav_list_2 = intake.word_substitute(modified_nav_list, [{'main':'ëª¨ë‹ì£½', 'sub_words':['ë‹¨í˜¸ë°•', 'ë‹¨í˜¸ë°•ì´', 'ê¿€ê³ êµ¬ë§ˆ']}])\n",
    "print(modified_nav_list_2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í† í”½ëª¨ë¸ë§ì€ ì¡°ì¸ëœ ê²ƒì´ í•„ìš”í•˜ë‹¤.\n",
    "### join_list ë¥¼ í†µí•´ì„œ ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake.morph_joined = intake.join_list(intake.nav_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labnosh.morph_joined = labnosh.join_list(labnosh.nav_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### íŒŒë¼ë¯¸í„°ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. \n",
    "#### self.make_lda(self, morph_joined, ntopic=10, learning_method='batch', max_iter=25, random_state=0, n_words=20)\n",
    "#### ntopicê³¼ n_wordsë¥¼ ë„£ì–´ì£¼ë©´ ëª‡ê°œì˜ í† í”½ìœ¼ë¡œ ë‚˜ëˆŒì§€, ëª‡ê°œì˜ ë‹¨ì–´ë¥¼ ë³´ì—¬ì¤„ì§€ë¥¼ ì •í•  ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       topic 3       topic 4       \n",
      "--------      --------      --------      --------      --------      \n",
      "ëª¨ë‹ì£½           í˜ë‚´            ìˆëŠ”            ë‹¤ì´ì–´íŠ¸          ê±´ê°•            \n",
      "ì•„ì¹¨            ìš´ë™            ë§ì€            ì €ë…            ë‹¤ì´ì–´íŠ¸          \n",
      "ì¼ìƒ            ì‹œì¼œ            ì…ë‹ˆ            ë‹¤ì´ì–´í„°          êµ¬ë§¤            \n",
      "ë¨¹ìŠ¤íƒ€ê·¸ë¨         ì•Šì•„            ìˆì–´            ì ì‹¬            ë¨¹ìŠ¤íƒ€ê·¸ë¨         \n",
      "ë‹¤ì´ì–´íŠ¸          ë¨¹ê²             ê°™ì•„            ë“¤ì–´ì˜¤ì‹œ          ë§›ìŠ¤íƒ€ê·¸ë¨         \n",
      "ë‹¨í˜¸ë°•           íƒ€ì„œ            ì´ë²¤íŠ¸           ë‹¤ì´ì–´íŠ¸ê·¸ë¨        ë§í¬            \n",
      "ê°„ì‹            ì£¼ë¬¸í–ˆ           ì”¨ì”¨ì•™           1ê°œ            ë°ì¼ë¦¬           \n",
      "ê³ êµ¬ë§ˆ           ì•ŠëŠ”            ì•„ë‹ˆ            ë¨¹ê³             ìœ ì‚°ê·            \n",
      "ë°ì¼ë¦¬           ì”¹ì–´            ê´œì°®            ë°›ì€            ê±´ê°•ì‹           \n",
      "ë§›ìŠ¤íƒ€ê·¸ë¨         ë“œì‹œ            ì—†ëŠ”            ì‚¬ê³¼            í›„ê¸°            \n",
      "ì†Œí†µ            ì°ì–´            ìˆë‹¤            í•¨ê»˜í•˜ëŠ”          ì„ ë¬¼            \n",
      "ë‹¨í˜¸ë°•ì£½          ì •ëª¨            ì°¸ì—¬            ê³ êµ¬ë§ˆ           ê°„ì‹            \n",
      "ì‹ë‹¨ì¡°ì ˆ          ë“¤ì–´            ë‹¹ì²¨            ì•„ì¹¨            í”„ë¡œí•„           \n",
      "ì•„ì¹¨ì‹ì‚¬          ë‹¤ë…¸            ë‹¤ì´ì–´íŠ¸          ê°„ì‹            ë§›ìˆëŠ”           \n",
      "ìš´ë™            ì§ì¥ì¸           ì˜ˆìœ            ë‹¤ì´ì–´íŠ¸ì‹ë‹¨        ìŠˆí¼ë°”           \n",
      "ì•„ì¹¨ë°¥           ë“¤ì–´ê°€           ê°™ë‹¤            ì˜¤ëŠ˜ì˜ì‹ë‹¨         ë¹„íƒ€ë¯¼           \n",
      "ë§íŒ”            ë²„ë¦¬            ê°™ì€            ì˜¤íŠ¸ë°€           ê²¬ê³¼ë¥˜           \n",
      "ëª¨ë‹            í™˜ì˜            ë„¤ì´ë²„           ê¸°ë¡            ì‹ë‹¨            \n",
      "ê°„í¸ì‹           ì±„ì›Œ            ìˆê³             ì‚¶ì€ê³„ë€          ì¼ìƒ            \n",
      "ëª¨ë‹ì£½ë‹¨í˜¸ë°•        í•˜ì„¸            ê·¸ëŸ°            ì‹ë‹¨ì¼ê¸°          í‘¸ë“œìŠ¤íƒ€ê·¸ë¨        \n",
      "\n",
      "\n",
      "topic 5       topic 6       topic 7       topic 8       topic 9       \n",
      "--------      --------      --------      --------      --------      \n",
      "íŒŒì›Œì ¤ë¶€ìŠ¤íŠ¸        í¡ê¸°            ë‹¤ì´ì–´íŠ¸          ë°€ìŠ¤            ë¨¹ì–´            \n",
      "ì•„ë¯¸ë…¸ë¦¬ì»¤ë²„        íŠœë‹            ë‹¤ì´ì–´í„°          ì‹ì‚¬ëŒ€ìš©          ë¨¹ì—ˆ            \n",
      "ìš´ë™            ì¹´ìŠ¤íƒ€ê·¸ë¨         ë‹¤ì´ì–´íŠ¸ì‹ë‹¨        ë‹¤ì´ì–´íŠ¸          ë¨¹ìœ¼            \n",
      "ë§›ìˆ            ìë™ì°¨           ì‹ë‹¨ì¼ê¸°          ë¯¸ë˜ì‹ì‚¬          ë¨¹ëŠ”            \n",
      "íŒŒì›Œì ¤           ë¬¸ì˜            ì ì‹¬            ìš°ìœ             ì±™ê²¨            \n",
      "í˜„ì •íˆ¬ì–´          ì„¸ë¸            ì €ë…            ë„ˆë¬´            ë¨¹ê³             \n",
      "ìš´ë™í•˜ëŠ”ì—¬ì        ì „í™”            ë‹¤ì´ì–´íŠ¸ê·¸ë¨        ì‹ì‚¬            ê°ì‚¬í•©           \n",
      "ìš´ë™í•˜ëŠ”ì§ì¥ì¸       ë¨¸í”ŒëŸ¬           ì‹ë‹¨            ì•„ì¹¨            ë“¤ì–´            \n",
      "ì—†ë‹¤            ë™ì˜ìƒ           ì•„ì¹¨            ì—ì„œ            ë„£ì–´            \n",
      "ëŸ¬ë‹            ê°€ë³€ë°°ê¸°          ë‘ìœ             ë°€ìŠ¤ë¼ì´íŠ¸         í•˜ëŠ”            \n",
      "ëª¸ìŠ¤í„°ì¦ˆ          ë¼ì´íŠ¸           ì˜¤ëŠ˜            í•´ì„œ            í•´ì•¼            \n",
      "ì»´í¬íŠ¸           ë ˆì´            ì¼ìƒ            ì¹¼ë¡œë¦¬           ë§ˆì‹œ            \n",
      "ëŸ°ìë§¤           íˆ¬ìŠ¤ì¹´ë‹ˆ          ê°„ì‹            ì˜¤ëŠ˜            ë‚¨ê²¨            \n",
      "í”„ë¦½            ìë™ì°¨ê·¸ë¨         ë‹¤ì´ì–´íŠ¸ì¼ê¸°        ë¯¸ìˆ«ê°€ë£¨          ì•Šê³             \n",
      "ì†Œë°±ì‚°           ìŠ¤íŒŒí¬           ë¨¹ìŠ¤íƒ€ê·¸ë¨         ëŠë‚Œ            ë³´ë‹ˆ            \n",
      "ì‚¬ì§„            í˜ì´ìŠ¤ë¶          ì†Œì´ë°€í¬          ì„ ì‹            ë„£ê³             \n",
      "ìš´ë™ìŠ¤íƒ€ê·¸ë¨        ì•„ë°˜í…Œmd         ì”¨ì”¨ì•™           ì‰ì´í¬           í•˜ê¸°            \n",
      "ì•„ì›ƒë„ì–´ì—‘ìŠ¤í¬ë£¨      ìˆœì •í˜•ê°€ë³€         ì˜¨ë”í…Œì´ë¸”         ê¹Œì§€            ë˜ëŠ”            \n",
      "ì´ë²¤íŠ¸           ì¹´ë§ˆë£¨           ë„ˆë¬´            ì½”ì½”ë„›           ë§Œë“¤ì–´           \n",
      "ëŸ°ìŠ¤íƒ€ê·¸ë¨         ë§›ì§‘            ì‹ë‹¨ê¸°ë¡          ì œí’ˆ            í•˜ì‹œ            \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "intake.LDA = SB_LDA()\n",
    "intake.LDA.make_lda(intake.morph_joined, ntopic=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labnosh.LDA = SB_LDA()\n",
    "labnosh.LDA.make_lda(labnosh.morph_joined, ntopic=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê·¸ í† í”½ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ 10ê°œë¥¼ ë³´ì—¬ì¤€ë‹¤. ë‘ë²ˆì§¸ ì¸ìëŠ” í† í”½ì˜ ì¸ë±ìŠ¤ì´ê³  ì„¸ë²ˆì§¸ ì¸ë±ìŠ¤ëŠ” ë³´ì—¬ì¤„ í¬ìŠ¤íŒ…ì˜ ê°œìˆ˜ì´ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic0 = intake.LDA.related_doc(intake.raw_data[:,2], 0, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic0 = labnosh.LDA.related_doc(labnosh.raw_data[:,2], 0, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë°ì´í„° í˜•ì‹ì€ ê°™ê³  í•  ì¤„ ì•Œê² ì§€?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake.SB_word2vec = SB_Word2Vec(intake.nav_list)\n",
    "\n",
    "intake.flavor = intake.SB_word2vec.get_sim_sen('ë§›', intake.raw_data[:,2], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labnosh.SB_word2vec = SB_Word2Vec(labnosh.nav_list)\n",
    "\n",
    "labnosh.flavor = labnosh.SB_word2vec.get_sim_sen('ë§›', labnosh.raw_data[:,2], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ë§›ì€', 0.6681041717529297),\n",
       " ('ì‹ê°', 0.6280479431152344),\n",
       " ('ìƒê°ë³´ë‹¤', 0.6160725355148315),\n",
       " ('ìƒˆì½¤ë‹¬ì½¤', 0.6123064756393433),\n",
       " ('ë¯¸ìˆ«ê°€ë£¨', 0.5867149829864502),\n",
       " ('ë‹¨ë§›', 0.5775174498558044),\n",
       " ('ë‹¨ì ', 0.5737367868423462),\n",
       " ('ê³ ì†Œ', 0.5660829544067383),\n",
       " ('ì¡°ê¸ˆ', 0.5660343170166016),\n",
       " ('ì•½ê°„', 0.5656855702400208)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intake.SB_word2vec.em.wv.most_similar('ë§›') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most_similar(positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)Â¶\n",
    "\n",
    "model.wv.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "[('queen', 0.50882536), ...]\n",
    "\n",
    "model.wv.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])\n",
    "[('queen', 0.71382287), ...]\n",
    "\n",
    "\n",
    "model.wv.doesnt_match(\"breakfast cereal dinner lunch\".split())\n",
    "'cereal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ë¯¸ìˆ«ê°€ë£¨', 0.5826406478881836),\n",
       " ('ìƒê°ë³´ë‹¤', 0.579865574836731),\n",
       " ('ì‹¬ì§€ì–´', 0.5629949569702148),\n",
       " ('ê·€ë¦¬', 0.5474948883056641),\n",
       " ('ì™„ì „', 0.5468454957008362),\n",
       " ('ì•„ì£¼', 0.5345592498779297),\n",
       " ('ë§›ë‚˜', 0.5310249328613281),\n",
       " ('ê·¸ëƒ¥', 0.5308135747909546),\n",
       " ('ì¼ë‹¨', 0.5180487036705017),\n",
       " ('ì¢‹ì•„', 0.5167618989944458)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intake.SB_word2vec.em.wv.most_similar(positive=['ì•„ì¹¨', 'ë§›'], negative=['ìš´ë™'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDFì—ëŠ” merge ëœ ë¦¬ìŠ¤íŠ¸ë¥¼ ë„£ì–´ì„œ ë¹„êµí•´ì£¼ë©´ ëœë‹¤. \n",
    "## intakeë§Œ ìˆì–´ì„œ íŒ¨ìŠ¤\n",
    "## í† í”½ëª¨ë¸ë§ ëœ ì• ë“¤ë¼ë¦¬ ë¹„êµí•´ë³´ëŠ” ê²ƒë„ ì¢‹ì„ ë“¯."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('í¡ê¸°', 0.25419146503600865),\n",
      " ('íŠœë‹', 0.23128150466795636),\n",
      " ('ì¢‹ì€', 0.2160081977559215),\n",
      " ('íŒŒì›Œì ¤ë¶€ìŠ¤íŠ¸', 0.18764348491928534),\n",
      " ('ì¹´ìŠ¤íƒ€ê·¸ë¨', 0.1800068314632679),\n",
      " ('ìë™ì°¨', 0.17346112850096726),\n",
      " ('ì•„ë¯¸ë…¸ë¦¬ì»¤ë²„', 0.1680063760323834),\n",
      " ('ë¨¹ìœ¼ë©´', 0.16364257405751628),\n",
      " ('ë‹¨íŒ¥ì£½', 0.13418691072716335),\n",
      " ('ë¨¸í”ŒëŸ¬', 0.13091405924601301)]\n",
      "\n",
      "[('ìœ¼ë¡œ', 0.4224923468565074),\n",
      " ('ì‡¼ì½œë¼', 0.34686938659387423),\n",
      " ('ë¯¸ì‹ë‹¹', 0.3169824681667258),\n",
      " ('ë§ì´', 0.22822737708004256),\n",
      " ('ë§›ì´', 0.19471780187384585),\n",
      " ('ê·¸ë˜ë†€ë¼ìš”ê±°íŠ¸', 0.18384983153670095),\n",
      " ('ììƒ‰ê³ êµ¬ë§ˆ', 0.18203850314717682),\n",
      " ('í‘¸ë“œì‰ì´í¬', 0.17162336490741298),\n",
      " ('ê·¸ë¦°ì”¨ë¦¬ì–¼', 0.16664221183622158),\n",
      " ('í•¨ê»˜', 0.13856662179859727)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf = SB_Tfidf([intake.nav_merged, labnosh.nav_merged])\n",
    "tfidf.get_tfidf()\n",
    "tfidf_of_all = tfidf.tfidf_hangul\n",
    "\n",
    "for i in tfidf_of_all:\n",
    "    pprint(i[:10])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abys\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "labnosh = Social_analysis('labnosh.txt', 'labnosh.official')\n",
    "\n",
    "meals = Social_analysis('meals.txt', 'intakefoods')\n",
    "\n",
    "morningjuk = Social_analysis('morningjuk.txt', 'intakefoods')\n",
    "\n",
    "easy_food = Social_analysis('easy_food.txt')\n",
    "\n",
    "conven_food = Social_analysis('conven_food.txt')\n",
    "\n",
    "keywords = [intake, labnosh, meals, morningjuk, easy_food, conven_food]\n",
    "\n",
    "### ë¹ˆë„ë¶„ì„\n",
    "\n",
    "for i in keywords:\n",
    "    print(len(i.main_text_list), len(i.morph_merged))\n",
    "\n",
    "labnosh.morph_frequency = analyzer.frequency(labnosh.morph_merged)\n",
    "pprint(labnosh.morph_frequency)\n",
    "\n",
    "meals.morph_frequency = analyzer.frequency(meals.morph_merged)\n",
    "pprint(meals.morph_frequency)\n",
    "\n",
    "morningjuk.morph_frequency = analyzer.frequency(morningjuk.morph_merged)\n",
    "pprint(morningjuk.morph_frequency)\n",
    "\n",
    "easy_food.morph_frequency = analyzer.frequency(easy_food.morph_merged)\n",
    "pprint(easy_food.morph_frequency)\n",
    "\n",
    "conven_food.morph_frequency = analyzer.frequency(conven_food.morph_merged)\n",
    "pprint(conven_food.morph_frequency)\n",
    "\n",
    "### í† í”½ëª¨ë¸ë§\n",
    "\n",
    "intake.LDA = analyzer.SB_LDA()\n",
    "intake.LDA.make_lda(intake.morph_joined, ntopic=10)\n",
    "\n",
    "topic0 = intake.LDA.related_doc(intake.main_text_list, 0)\n",
    "\n",
    "topic1 = intake.LDA.related_doc(intake.main_text_list, 1)\n",
    "\n",
    "topic2 = intake.LDA.related_doc(intake.main_text_list, 2)\n",
    "\n",
    "topic3 = intake.LDA.related_doc(intake.main_text_list, 3)\n",
    "\n",
    "topic7 = intake.LDA.related_doc(intake.main_text_list, 7)\n",
    "\n",
    "topic9 = intake.LDA.related_doc(intake.main_text_list, 9)\n",
    "\n",
    "topic3 = intake.LDA.related_doc(intake.main_text_list, 3)\n",
    "\n",
    "\n",
    "\n",
    "labnosh.LDA = analyzer.SB_LDA()\n",
    "labnosh.LDA.make_lda(labnosh.morph_joined, ntopic=10)\n",
    "\n",
    "meals.LDA = analyzer.SB_LDA()\n",
    "meals.LDA.make_lda(meals.morph_joined, ntopic=10)\n",
    "\n",
    "morningjuk.LDA = analyzer.SB_LDA()\n",
    "morningjuk.LDA.make_lda(morningjuk.morph_joined, ntopic=10)\n",
    "\n",
    "easy_food.LDA = analyzer.SB_LDA()\n",
    "easy_food.LDA.make_lda(easy_food.morph_joined, ntopic=10)\n",
    "\n",
    "conven_food.LDA = analyzer.SB_LDA()\n",
    "conven_food.LDA.make_lda(conven_food.morph_joined, ntopic=10)\n",
    "\n",
    "### Word2Vec\n",
    "\n",
    "intake.SB_word2vec = analyzer.SB_Word2Vec(intake.hashtags_appended)\n",
    "\n",
    "intake.flavor = intake.SB_word2vec.get_sim_sen('ë§›', intake.main_hash_dic, intake.main_text_list, 4)\n",
    "\n",
    "\n",
    "intake.diet = intake.SB_word2vec.get_sim_sen('ë‹¤ì´ì–´íŠ¸', intake.main_hash_dic, intake.main_text_list, 4)\n",
    "\n",
    "\n",
    "intake.health = intake.SB_word2vec.get_sim_sen('ê±´ê°•', intake.main_hash_dic, intake.main_text_list, 4)\n",
    "\n",
    "\n",
    "intake.exercise = intake.SB_word2vec.get_sim_sen('ìš´ë™', intake.main_hash_dic, intake.main_text_list, 4)\n",
    "\n",
    "\n",
    "intake.morningjuk = intake.SB_word2vec.get_sim_sen('ëª¨ë‹ì£½', intake.main_hash_dic, intake.main_text_list, 4)\n",
    "\n",
    "\n",
    "intake.meals = intake.SB_word2vec.get_sim_sen('ë°€ìŠ¤', intake.main_hash_dic, intake.main_text_list, 4)\n",
    "\n",
    "\n",
    "intake.gonyak = intake.SB_word2vec.get_sim_sen('ê³¤ì•½ì ¤ë¦¬', intake.main_hash_dic, intake.main_text_list, 4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### TFIDF\n",
    "\n",
    "tfidf = analyzer.SB_Tfidf([intake.morph_merged, labnosh.morph_merged, meals.morph_merged, morningjuk.morph_merged, easy_food.morph_merged, conven_food.morph_merged])\n",
    "\n",
    "tfidf_of_all = tfidf.tfidf_hangul\n",
    "\n",
    "for i in tfidf_of_all:\n",
    "    pprint(i[:10])\n",
    "    print()\n",
    "\n",
    "meals_juk_SB_Tfidf = analyzer.SB_Tfidf([meals.morph_merged, morningjuk.morph_merged])\n",
    "\n",
    "tfidf_meals_juk = meals_juk_SB_Tfidf.tfidf_hangul\n",
    "\n",
    "for i in tfidf_meals_juk:\n",
    "    pprint(i[:10])\n",
    "    print()\n",
    "\n",
    "intake_meals_SB_Tfidf = analyzer.SB_Tfidf([intake.morph_merged, meals.morph_merged])\n",
    "\n",
    "tfidf_intake_meals = intake_meals_SB_Tfidf.tfidf_hangul\n",
    "\n",
    "for i in tfidf_intake_meals:\n",
    "    pprint(i[:10])\n",
    "    print()\n",
    "\n",
    "intake_morningjuk_SB_Tfidf = analyzer.SB_Tfidf([intake.morph_merged, morningjuk.morph_merged])\n",
    "\n",
    "tfidf_intake_morningjuk = intake_morningjuk_SB_Tfidf.tfidf_hangul\n",
    "\n",
    "for i in tfidf_intake_morningjuk:\n",
    "    pprint(i[:10])\n",
    "    print()\n",
    "\n",
    "meals_conven_food_SB_Tfidf = analyzer.SB_Tfidf([meals.morph_merged, conven_food.morph_merged])\n",
    "\n",
    "tfidf_meals_conven_food = meals_conven_food_SB_Tfidf.tfidf_hangul\n",
    "\n",
    "for i in tfidf_meals_conven_food:\n",
    "    pprint(i[:10])\n",
    "    print()\n",
    "\n",
    "morningjuk_conven_food_SB_Tfidf = analyzer.SB_Tfidf([morningjuk.morph_merged, conven_food.morph_merged])\n",
    "\n",
    "tfidf_morningjuk_conven_food = morningjuk_conven_food_SB_Tfidf.tfidf_hangul\n",
    "\n",
    "for i in tfidf_meals_conven_food:\n",
    "    pprint(i[:10])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('morph_list.csv', 'w', encoding='utf-8') as f:\n",
    "    spamwriter = csv.writer(f, delimiter=' ',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    for i in intake.morph_list:\n",
    "        spamwriter.writerow(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morph_pos(text_list, add_list, del_list, exception_list = ['ë§›', 'ë°¥', 'ë¬¼', 'ëª¸']):\n",
    "    morph_list = []\n",
    "    noun_list = []\n",
    "    adj_list = []\n",
    "    verb_list = []\n",
    "\n",
    "    for j in text_list:\n",
    "        parsed = self.twitter.pos(j)\n",
    "        temp = []\n",
    "        n_temp = []\n",
    "        adj_temp = []\n",
    "        verb_temp = []\n",
    "\n",
    "        for i in parsed:\n",
    "            if self.isHangul(i):\n",
    "                if not i[0] in del_list:\n",
    "                    if len(i[0]) > 1:\n",
    "                        temp.append(i)\n",
    "                        if i[1] == 'Noun':\n",
    "                            n_temp.append(i[0])\n",
    "                        elif i[1] == 'Verb':\n",
    "                            n_adj.append(i[0])\n",
    "                        elif i[1] == 'Adjective':\n",
    "                            n_verb.append(i[0])\n",
    "\n",
    "                    elif i in exception_list:\n",
    "                        temp.append(i)\n",
    "            else: print(i, 'í•œê¸€ì´ ì•„ë‹™ë‹ˆë‹¤.')\n",
    "\n",
    "        morph_list.append(temp)\n",
    "        noun_list.append(n_temp)\n",
    "        adj_list.append(adj_temp)\n",
    "        verb_list.append(verb_temp)\n",
    "\n",
    "\n",
    "    nav_list = noun_list + adj_list + verb_list\n",
    "\n",
    "    return morph_list, nav_list, noun_list, adj_list, verb_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    '''def get_from_dic(self, filename):\n",
    "        self.data = handler.load_by_pickle(filename)\n",
    "        self.dataset, self.dataset_dic, self.main_text_list, self.morph_list, self.morph_merged, self.morph_joined, self.hashtags_merged, self.hashtags_appended, self.main_hash_dic = handler.create_datasets_with_dic(self.data)\n",
    "\n",
    "        # dataset, main_text_list, morph_list, morph_merged, morph_joined, hashtags_merged, hashtags_appended, main_hash_dic\n",
    "    def get_data_from_list(self, filename, add_list, del_list):\n",
    "        self.data = handler.load_by_pickle(filename)\n",
    "        self.dataset, self.main_text_list, self.morph_list, self.morph_merged, self.morph_joined, self.hashtags_merged, self.hashtags_appended, self.main_hash_dic = handler.create_datasets_with_list(self.data[1:], add_list, del_list)'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
