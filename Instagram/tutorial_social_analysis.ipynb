{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abys\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from ckonlpy.tag import Twitter\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.corpora import Dictionary\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import mglearn\n",
    "from pprint import pprint\n",
    "from ckonlpy.tag import Twitter\n",
    "import numpy as np\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "class Social_analysis():\n",
    "    \n",
    "    non_bmp_map = dict.fromkeys(range(0x10000, sys.maxunicode + 1), 0xfffd)\n",
    "\n",
    "    def __init__(self):\n",
    "        self.twitter = Twitter()\n",
    "        \n",
    "    def pickle_to_table(self, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        data = data[1:]\n",
    "        for idx, i in enumerate(data):\n",
    "            data[idx][2] = i[2].replace('#', ' ').translate(self.non_bmp_map)\n",
    "            data[idx][3] = '/'.join(i[3])\n",
    "            data[idx][4] = '/'.join(i[4])\n",
    "        self.raw_data = np.array(data)\n",
    "  \n",
    "    def hashtags_split(self, hashtags):        \n",
    "        hashtags_split = []\n",
    "        for i in hashtags:\n",
    "            hashtags_split.append(i.split('/'))\n",
    "        \n",
    "        hashtags_list = []\n",
    "        \n",
    "        for i in hashtags_split:\n",
    "            temp = []\n",
    "            for j in i:\n",
    "                if self.isHangul(j):\n",
    "                    t_hashtags = j.translate(self.non_bmp_map)\n",
    "                    temp.append(t_hashtags)\n",
    "            hashtags_list.append(temp)\n",
    "        self.hashtags_list = hashtags_list\n",
    "        \n",
    "        return hashtags_list\n",
    "                    \n",
    "    def add_keyword_dic(self, keyword_list, tag='Noun'):\n",
    "        for i in keyword_list:\n",
    "            if type(i) == tuple:\n",
    "                self.twitter.add_dictionary(i[0], i[1])\n",
    "            else:\n",
    "                self.twitter.add_dictionary(i, tag)\n",
    "        \n",
    "    def morph_pos(self, text_list, exception_list = ['맛', '밥', '물', '몸']):\n",
    "        \n",
    "        morph_list = []\n",
    "        noun_list = []\n",
    "        adj_list = []\n",
    "        verb_list = []\n",
    "        \n",
    "        for j in text_list:\n",
    "            parsed = self.twitter.pos(j)\n",
    "            temp = []\n",
    "            n_temp = []\n",
    "            adj_temp = []\n",
    "            verb_temp = []\n",
    "            \n",
    "            for i in parsed:\n",
    "                if self.isHangul(i[0]):\n",
    "                    if ((len(i[0]) > 1) or (i[0] in exception_list)):\n",
    "                        temp.append(i)\n",
    "                        if i[1] == 'Noun':\n",
    "                            n_temp.append(i[0])\n",
    "                        elif i[1] == 'Verb':\n",
    "                            adj_temp.append(i[0])\n",
    "                        elif i[1] == 'Adjective':\n",
    "                            verb_temp.append(i[0])\n",
    "                    else:\n",
    "                        print('{} 제외'.format(i[0]))\n",
    "                else: print('{} 한글이 아님.'.format(i[0]))\n",
    "            \n",
    "\n",
    "            morph_list.append(temp)\n",
    "            noun_list.append(n_temp)\n",
    "            adj_list.append(adj_temp)\n",
    "            verb_list.append(verb_temp)\n",
    "\n",
    "        nav_list = noun_list + adj_list + verb_list\n",
    "\n",
    "        return morph_list, nav_list, noun_list, adj_list, verb_list\n",
    "\n",
    "\n",
    "    \n",
    "    def pos_extractor(self, parsed):\n",
    "        for i in parsed:            \n",
    "            temp = []\n",
    "            n_temp = []\n",
    "            adj_temp = []\n",
    "            verb_temp = []\n",
    "            if self.isHangul(i[0]):\n",
    "                if ((len(i[0]) > 1) or (i[0] in exception_list)):\n",
    "                    temp.append(i)\n",
    "                    if i[1] == 'Noun':\n",
    "                        n_temp.append(i[0])\n",
    "                    elif i[1] == 'Verb':\n",
    "                        adj_temp.append(i[0])\n",
    "                    elif i[1] == 'Adjective':\n",
    "                        verb_temp.append(i[0])\n",
    "                else:\n",
    "                    print('{} 제외'.format(i[0]))\n",
    "            else: print('{} 한글이 아님.'.format(i[0]))\n",
    "            \n",
    "\n",
    "            morph_list.append(temp)\n",
    "            noun_list.append(n_temp)\n",
    "            adj_list.append(adj_temp)\n",
    "            verb_list.append(verb_temp)\n",
    "\n",
    "        nav_list = noun_list + adj_list + verb_list\n",
    "\n",
    "        return morph_list, nav_list, noun_list, adj_list, verb_list\n",
    "\n",
    "    \n",
    "    \n",
    "    def merge_list(self, tokenized_list):\n",
    "        return [j for i in tokenized_list for j in i]\n",
    "\n",
    "    \n",
    "    def join_list(self, tokenized_list):\n",
    "        joined_list = []\n",
    "        for idx, i in enumerate(tokenized_list):\n",
    "            joined_list.append(\" \".join(i))\n",
    "        return joined_list\n",
    " \n",
    "    def split_list(self, untokenized_list):\n",
    "        hashtag_splited = []\n",
    "        for idx, i in enumerate(untokenized):\n",
    "            hashtag_splited.append(i.split('/'))\n",
    "            return hastag_splited\n",
    "\n",
    "    def word_substitute(self, dataset, sublist):\n",
    "        dataset = copy.deepcopy(dataset)\n",
    "        sub_book = dict()\n",
    "        for i in sublist:\n",
    "            for j in i['sub_words']:\n",
    "                sub_book[j] = i['main']\n",
    "        gc.collect()\n",
    "        for n, i in enumerate(dataset):\n",
    "            dataset[n] = [sub_book.get(item,item) for item in i]\n",
    "\n",
    "        del sub_book\n",
    "        gc.collect()\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    def word_delete(self, dataset, del_list):\n",
    "        dataset = copy.deepcopy(dataset)\n",
    "\n",
    "        for n, line in enumerate(dataset):\n",
    "             dataset[n] = [i for i in line if i not in del_list]\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    \n",
    "    def isHangul(self, text):\n",
    "        encText = text\n",
    "        hanCount = len(re.findall(u'[\\u3130-\\u318F\\uAC00-\\uD7A3]+', encText))\n",
    "        return hanCount > 0\n",
    "\n",
    "    \n",
    "\n",
    "class SB_Word2Vec():    \n",
    "    \n",
    "    def __init__(self, morph_list):\n",
    "        self.dct = Dictionary(morph_list)\n",
    "        self.corpus = [self.dct.doc2bow(line) for line in morph_list]\n",
    "        self.build_Word2Vec(morph_list)\n",
    "    \n",
    "    def make_Word2Vec(self, morph_list, size=50, window=2, min_count=10, iteration=100):\n",
    "        self.em = Word2Vec(morph_list, size=size, window=window, min_count=min_count, iter=iteration)\n",
    "        self.em_vocab = list(self.em.wv.vocab.keys())\n",
    "        self.em_vocab_dic = {word:idx for idx, word in enumerate(self.em_vocab)}\n",
    "\n",
    "    def make_Word2Sen_matrix(self): \n",
    "        vocab_size = len(self.em_vocab)\n",
    "        self.sen_matrix = np.zeros((len(self.corpus), vocab_size))\n",
    "        for idx, row in enumerate(self.sen_matrix):\n",
    "            for idx2, frequency in self.corpus[idx]:\n",
    "                    if self.dct[idx2] in self.em_vocab:\n",
    "                        self.sen_matrix[idx][self.em_vocab_dic[self.dct[idx2]]] = frequency                \n",
    "        self.sim_matrix = np.zeros((vocab_size, vocab_size))\n",
    "        for idx, w1 in enumerate(self.em_vocab):\n",
    "            for idx2, w2 in enumerate(self.em_vocab):\n",
    "                self.sim_matrix[idx][idx2] =  self.em.wv.similarity(w1, w2)\n",
    "\n",
    "        self.word2sen_matrix = np.dot(self.sim_matrix, np.transpose(self.sen_matrix))\n",
    "\n",
    "        return self.word2sen_matrix\n",
    "\n",
    "    def get_sim_sen(self, keyword, main_text, number=1):\n",
    "        self.sim_sen_index = np.argsort(self.word2sen_matrix[self.em_vocab_dic[keyword]])\n",
    "        self.most_sim_sen_index = np.argmax(self.word2sen_matrix[self.em_vocab_dic[keyword]])\n",
    "        index_list = self.sim_sen_index.reshape((-1,)).tolist()\n",
    "        index_list.reverse()\n",
    "        \n",
    "        for idx, i in enumerate(index_list[:number]):\n",
    "            print(str(idx + 1))\n",
    "            print(main_text[i])\n",
    "        return index_list\n",
    "    \n",
    "    def build_Word2Vec(self, morph_list):\n",
    "        self.make_Word2Vec(morph_list)\n",
    "        self.make_Word2Sen_matrix()\n",
    "        \n",
    "        \n",
    "class SB_LDA():\n",
    "\n",
    "    def make_lda(self, morph_joined, ntopic=10, learning_method='batch', max_iter=25, random_state=0, n_words=20):        \n",
    "        self.vect = CountVectorizer(max_features=10000, max_df=.15)\n",
    "        self.X = self.vect.fit_transform(morph_joined)\n",
    "        self.lda = LatentDirichletAllocation(n_components=ntopic, learning_method=learning_method, max_iter=max_iter, random_state=random_state)\n",
    "        self.document_topics = self.lda.fit_transform(self.X)\n",
    "        self.sorting = np.argsort(self.lda.components_, axis=1)[:, ::-1]\n",
    "        self.feature_names = np.array(self.vect.get_feature_names())\n",
    "        mglearn.tools.print_topics(topics=range(ntopic), feature_names=self.feature_names, sorting=self.sorting, topics_per_chunk=5, n_words=n_words)\n",
    "\n",
    "    def related_doc(self, main_text_list, topic_index, number=10):\n",
    "        category = np.argsort(self.document_topics[:, topic_index])[::-1]\n",
    "        related_docs = []\n",
    "        for i in category[:number]:\n",
    "            print(i)\n",
    "            print(main_text_list[i] + \".\\n\")\n",
    "            related_docs.append((i, main_text_list[i]))\n",
    "        return related_docs\n",
    "\n",
    "class SB_Tfidf():    \n",
    "    \n",
    "    def __init__(self, list_morph_merged):\n",
    "        self.list_morph_merged = list_morph_merged\n",
    "        self.dct = Dictionary(self.list_morph_merged)\n",
    "        self.corpus = [self.dct.doc2bow(line) for line in self.list_morph_merged]\n",
    "\n",
    "    def get_tfidf(self):       \n",
    "        self.model = TfidfModel(self.corpus)\n",
    "        self.tfidf = []\n",
    "        for i in self.corpus:\n",
    "             self.tfidf.append(sorted(self.model[i], key = lambda x: x[1], reverse=True))\n",
    "        self.tfidf_hangul = []\n",
    "        for idx1, i in enumerate(self.tfidf):\n",
    "            self.tfidf_hangul.append([(self.dct[j[0]], j[1]) for j in i])        \n",
    "        \n",
    "        return self.tfidf_hangul\n",
    "    \n",
    "def frequency(merged):\n",
    "    word_count = Counter(merged)\n",
    "    word_count2 = []\n",
    "    for i in word_count:\n",
    "        word_count2.append((i, word_count[i]))\n",
    "    word_count2 = sorted(word_count2, key=lambda x: x[1], reverse = True)\n",
    "    return word_count2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake = Social_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labnosh = Social_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB에서 불러온 데이터를 Social_analysis 인스턴스에 등록해도  되고,  그냥 그 데이터로 앞으로 진행해도 된다. 일단 튜토리얼은 pickle 데이터로 진행하기 때문에 아래 함수를 실행하면 intake.row_data 에 저장이 된다. 이 놈은 DB에 들어간 놈이 비슷하게 생긴 놈이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake.pickle_to_table('Data/intake_list.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labnosh.pickle_to_table('Data/labnosh_list.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석에 꼭 포함하고 싶은 단어를 추가한다. 기본적으로는 hashtag들을 noun으로 추가한다. 여기서 해쉬태그 중에 포함하고 싶지 않은 것을 빼는 것을 추천한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5235,)\n",
      "['아침식사', '죽', '모닝죽', '단호박', '인테이크', '맛스타그램', '먹스타그램', '식사대용', '다이어트', '두통', '좋아요반사', '맛집', '간편식', '아점', '아픔', '선팔은맞팔', '맛스타', '선팔', '맞팔', '먹스타', '음식', '푸드스타그램', '좋반']\n"
     ]
    }
   ],
   "source": [
    "intake.hashtag_splited = intake.hashtags_split(intake.raw_data[:, 3])\n",
    "print(np.shape(intake.hashtag_splited))\n",
    "print(intake.hashtag_splited[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7967,)\n",
      "['와이식단', '와이_받았다그램', '에잇템', '푸드바', '푸드바', '프로틴바', '푸드바', '푸드바', '에잇템', '랩노쉬', '초코바', '프로틴', '다이어트', '식단', '다이어트식단', '식단관리', '건강식', '다이어트그램', '다이어터그램', '다이어트계정', '다이어트소통', '다이어트기록']\n"
     ]
    }
   ],
   "source": [
    "labnosh.hashtag_splited = labnosh.hashtags_split(labnosh.raw_data[:, 3])\n",
    "print(np.shape(labnosh.hashtag_splited))\n",
    "print(labnosh.hashtag_splited[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['user_id', '2018-05-03T23:42:16.000Z',\n",
       "        'y_diet__ 20180504 오늘의 아침�\\n 와이식단  와이_받았다그램\\n✔️ 에잇템  푸드바\\n✔️우유 한 잔\\n에잇템 이벤트로 받게 된 신제품‼️  푸드바 ��\\n일단 총평은! 요즘 말로 \"갓 띵 템 ��\\u200d♀️\" 일단 다이어트하면서 초코 좋아하시는분들이면 당장 사주세요! 브라우니 먹는 느낌, 아니면 시중에 파는 초코바 먹는 느낌! 너무 꾸덕, 단단하지도 않으면서 막 촉촉하지도 않은 중간 정도의 식감에 중간 중간 초콜릿이 박혀있고, 코코넛까지 들어있어서 식감 최고!��\\n95% 이상이 유기농 원료로 만들어졌기 때문에 믿고 안심하고 먹을 수 있고, 200kcal에 식이섬유가11g (44%), 단백질 13g(24%) 등 영양성분도 최고..!��\\n당류는 9g(9%) 정도인데, 개인적으로 단 걸 좋아하는 편이 아니라서 푸드바를 다 먹어갈때쯤 달아서 조금 물린다(?)는 느낌을 받았지만! 그럴 땐 우유 한 모금 마셔주면 다시 홀린듯이 찾게 되는 마성의 매력을 가진 푸드바✨\\n또다른 신제품 푸드쉐이크도 같이 보내주셨길래 감동받아서 지기님께 디엠 보냈더니 넘 스윗한 답장 보내주셔서 심쿵❤️\\n아무튼 이거 진짜 최고  프로틴바  푸드바 ‼️ 다 먹으면 무조건 재구매 할거에요!! 초코 좋아하시는 다이어터라면 꼭 드셔보세요 ! 후회하지 않으실거에요�\\n간단한 한 끼 식사로, 혹은 오후에 당 떨어질 때 간식으로 먹으면 딱 좋을 아이템입니다! 이런 좋은 제품 만들어 준 에잇템에게 감사를!!��❤️',\n",
       "        '20180504/와이식단/와이_받았다그램/에잇템/푸드바/푸드바/프로틴바/푸드바/푸드바/에잇템/랩노쉬/초코바/프로틴/다이어트/식단/다이어트식단/식단관리/foodbar/diet/eatclean/건강식/다이어트그램/다이어터그램/다이어트계정/다이어트소통/다이어트기록/bbin_d/young.luvv/im_sooki/juning_diet/__diet_bin/h.jin_0103/nari2323',\n",
       "        'y_diet__#푸드바 #에잇템 #랩노쉬 #초코바 #프로틴 #다이어트 #식단 #다이어트식단 #식단관리 #foodbar #diet #eatclean #건강식 #다이어트그램 #다이어터그램 #다이어트계정 #다이어트소통 #다이어트기록/oklmbbin푸드바 궁금하네오 쫀득해보이뉸.../tjdworb5오늘도 좋은하루!!/young.luvv헐랭방구 이거슨 저를 위한건가요/im_sooki요놈보게.. 엄청난 꾸덕달달건강만점이 일것 같아여!!!♥ 에잇템 구경가봐야겠어요! ㅋㅋㅋㅋㅋ/y_diet__@bbin_d 진짜 최고에요ㅠㅠ 막 쫀득해서 이에 달라붙는 것도 아니구 적당히 쬰득촉촉..♥/y_diet__@young.luvv 영님 ㅠ 구매각이에여!!!! 저도 다먹고 구매하려구욘👀❤️/y_diet__@im_sooki 수키님 정확해옄ㅋㅋㅋㅋㅋ꾸덕하고 달달하면서 건강만점! ㅠ 이런거 먹으면서 다이어트해도 되나 싶을정도의 맛이에요👍🏻❤️/juning_diet그냥 보기만해두 든든할것 같은 푸드바이네요 💚💚💚/__diet_bin와 비주얼부터가 찐득하니 운동 전 에 간식대용으로 진짜 최고일꺼 같아여 !!! 꺄/h.jin_0103완전..꾸덕...?!...가격이..😳😂⁉️/y_diet__@juning_diet 진짜 초코좋아하시는분들은 꼭 사야됩니당 ㅠㅠ\\U0001f92d♥️/y_diet__@__diet_bin 비니님 진쨔 최고에욘.. 초코초코 꾸덕쬰득촉촉..♥/y_diet__@h.jin_0103 10개입에 24,000원이에요!!/lee_suyeon91@nari2323 대박..',\n",
       "        None,\n",
       "        'https://www.instagram.com/p/BiVYjk4AWhX/?hl=ko&tagged=%EB%9E%A9%EB%85%B8%EC%89%AC'],\n",
       "       ['user_id', '2018-07-03T22:17:08.000Z',\n",
       "        'csmansoo2075빵순이 떡순이 밀가루중독자 군것질덕후인 저에게 건강한 간식이 도착했어요^^\\n그때그때 골라 먹을 수 있는 다섯가지 맛 푸드쉐이크 스타터키트와 꾸덕꾸덕 너무 맛있는 단백질 가득 푸드바!\\n푸드쉐이크는 플라스틱 용기로 되어 있어 물이나 우유 부어서 먹기 너무 편하고 푸드바는 초코초코 맛도 그렇고 꾸덕꾸덕 건강한 브라우니 먹는것 같은 느낌이네요\\n제가 너무 좋아하는 맛이에요^^\\n휴대가 간편한 푸드바랑 푸드쉐이크 하나씩 가방에 쏘옥~ 챙겨 나가서 입 심심할 때 건강한 간식이나 한끼 식사로 먹으려구요^^\\n인친님들도 건강한하루 보내세요��\\n.\\n.\\n.\\n� 랩노쉬 @labnosh.official\\n 스타터키트 푸드쉐이크 단백질쉐이크 선식 푸드바 단백질바 아침밥 다이어트 다이어트식단 다이어트요리 클린푸드 브런치 집밥 혼밥 홈쿡 홈메이드 온더테이블 플레이팅 킨포크 맛스타그램 먹스타그램 요리스타그램 집밥스타그램 onthetable eatclean cleanfood instafood foodstagram',\n",
       "        '랩노쉬/labnosh.official/스타터키트/푸드쉐이크/단백질쉐이크/선식/푸드바/단백질바/아침밥/다이어트/다이어트식단/다이어트요리/클린푸드/브런치/집밥/혼밥/홈쿡/홈메이드/온더테이블/플레이팅/킨포크/맛스타그램/먹스타그램/요리스타그램/집밥스타그램/onthetable/eatclean/cleanfood/instafood/foodstagram',\n",
       "        '', None,\n",
       "        'https://www.instagram.com/p/BkyTTcEgfBZ/?hl=ko&tagged=%EB%9E%A9%EB%85%B8%EC%89%AC']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labnosh.raw_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## intake.add_dic_keyword() 함수는 1차원 list를 받는다.\n",
    "## 데이터의 형태는 ['단어', '단어', '단어'] or [('단어', Tag), ('단어', Tag)]\n",
    "## Tag는 'Noun', 'Verb', 'Adjective' 등이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake.hashtag_merged = intake.merge_list(intake.hashtag_splited)\n",
    "\n",
    "intake.add_keyword_dic(intake.hashtag_splited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labnosh.hashtag_merged = labnosh.merge_list(labnosh.hashtag_splited)\n",
    "\n",
    "labnosh.add_keyword_dic(labnosh.hashtag_splited)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석을 진행한다. 한글자는 제외시키는데 혹시 필요하다고 판단되는 글자는 exception_list 파라미터에 리스트로 넣어주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake.morph_list, intake.nav_list, intake.noun_list, intake.adj_list, intake.verb_list= intake.morph_pos(intake.raw_data[:, 2], exception_list=['맛', '밥', '물', '몸', '죽'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labnosh.morph_list, labnosh.nav_list, labnosh.noun_list, labnosh.adj_list, labnosh.verb_list= labnosh.morph_pos(labnosh.raw_data[:, 2], exception_list=['맛', '밥', '물', '몸', '죽'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 빈도분석에는 merge된 list가 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('테이크', 6618), ('다이어트', 4639), ('그램', 4087), ('스타', 2790), ('죽', 2511)]\n"
     ]
    }
   ],
   "source": [
    "intake.nav_merged = intake.merge_list(intake.nav_list)\n",
    "\n",
    "intake.nav_frequency = frequency(intake.nav_merged)\n",
    "\n",
    "print(intake.nav_frequency [:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('랩노쉬', 10886), ('다이어트', 2730), ('맛', 1694), ('미래형식사', 1628), ('올리브영', 1355)]\n"
     ]
    }
   ],
   "source": [
    "labnosh.nav_merged = labnosh.merge_list(labnosh.nav_list)\n",
    "\n",
    "labnosh.nav_frequency = frequency(labnosh.nav_merged)\n",
    "\n",
    "print(labnosh.nav_frequency [:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 여기서 내가 형태소분석시에 안짤리게 할 단어와 제외하고 싶은 단어, 대체하고 싶은 단어를 선택한다.\n",
    "## 안짤리게 할 단어는 다시 위로 돌아가 추가한다.\n",
    "## 대체하고 싶은 단어와 제외하고 싶은 단어는 아래 방법으로 처리한다. 일단 생략한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분석에서 제외할 단어를 리스트로 넣어준다. 인자로 넣어주는 값은 변하지 않으므로 넣어보고 마음에 안들면 다시 intake.nav_list 로 접근하면 삭제 전 파일로 돌아갈 수 있다.\n",
    "## 진짜 후회없을 것 같을 때, intake.nav_list에 업데이트 하도록 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "intake.nav_frequency = frequency(intake.nav_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('인테이크', 5727), ('다이어트', 2167), ('밀스', 1490), ('아침', 1273), ('모닝죽', 1212)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intake.nav_frequency [:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아침식사', '죽', '모닝죽', '단호박', '인테이크', '모닝죽', '아침', '기도', '입맛', '하나', '아침', '뚝딱', '맛은', '종류', '맛있는데', '단호박이', '꿀고구마']\n"
     ]
    }
   ],
   "source": [
    "modified_nav_list = intake.word_delete(intake.nav_list, ['요거', '여러', '굳이'])\n",
    "print(modified_nav_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이번에는 단어를 대체하는 것, 리스트 안에 딕셔너리 형식\n",
    "#### [{'main': '최종적으로 대체하고 싶은 단어', 'sub_words':['대체할 단어', '대체할 단어']},\n",
    "####  {'main': '최종적으로 대체하고 싶은 단어', 'sub_words':['대체할 단어', '대체할 단어']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아침식사', '죽', '모닝죽', '모닝죽', '인테이크', '모닝죽', '아침', '기도', '입맛', '하나', '아침', '뚝딱', '맛은', '종류', '맛있는데', '모닝죽', '모닝죽']\n"
     ]
    }
   ],
   "source": [
    "modified_nav_list_2 = intake.word_substitute(modified_nav_list, [{'main':'모닝죽', 'sub_words':['단호박', '단호박이', '꿀고구마']}])\n",
    "print(modified_nav_list_2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토픽모델링은 조인된 것이 필요하다.\n",
    "### join_list 를 통해서 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake.morph_joined = intake.join_list(intake.nav_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labnosh.morph_joined = labnosh.join_list(labnosh.nav_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파라미터는 다음과 같다. \n",
    "#### self.make_lda(self, morph_joined, ntopic=10, learning_method='batch', max_iter=25, random_state=0, n_words=20)\n",
    "#### ntopic과 n_words를 넣어주면 몇개의 토픽으로 나눌지, 몇개의 단어를 보여줄지를 정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       topic 3       topic 4       \n",
      "--------      --------      --------      --------      --------      \n",
      "모닝죽           힘내            있는            다이어트          건강            \n",
      "아침            운동            많은            저녁            다이어트          \n",
      "일상            시켜            입니            다이어터          구매            \n",
      "먹스타그램         않아            있어            점심            먹스타그램         \n",
      "다이어트          먹겠            같아            들어오시          맛스타그램         \n",
      "단호박           타서            이벤트           다이어트그램        링크            \n",
      "간식            주문했           씨씨앙           1개            데일리           \n",
      "고구마           않는            아니            먹고            유산균           \n",
      "데일리           씹어            괜찮            받은            건강식           \n",
      "맛스타그램         드시            없는            사과            후기            \n",
      "소통            찍어            있다            함께하는          선물            \n",
      "단호박죽          정모            참여            고구마           간식            \n",
      "식단조절          들어            당첨            아침            프로필           \n",
      "아침식사          다노            다이어트          간식            맛있는           \n",
      "운동            직장인           예쁜            다이어트식단        슈퍼바           \n",
      "아침밥           들어가           같다            오늘의식단         비타민           \n",
      "맞팔            버리            같은            오트밀           견과류           \n",
      "모닝            환영            네이버           기록            식단            \n",
      "간편식           채워            있고            삶은계란          일상            \n",
      "모닝죽단호박        하세            그런            식단일기          푸드스타그램        \n",
      "\n",
      "\n",
      "topic 5       topic 6       topic 7       topic 8       topic 9       \n",
      "--------      --------      --------      --------      --------      \n",
      "파워젤부스트        흡기            다이어트          밀스            먹어            \n",
      "아미노리커버        튜닝            다이어터          식사대용          먹었            \n",
      "운동            카스타그램         다이어트식단        다이어트          먹으            \n",
      "맛있            자동차           식단일기          미래식사          먹는            \n",
      "파워젤           문의            점심            우유            챙겨            \n",
      "현정투어          세븐            저녁            너무            먹고            \n",
      "운동하는여자        전화            다이어트그램        식사            감사합           \n",
      "운동하는직장인       머플러           식단            아침            들어            \n",
      "없다            동영상           아침            에서            넣어            \n",
      "러닝            가변배기          두유            밀스라이트         하는            \n",
      "몸스터즈          라이트           오늘            해서            해야            \n",
      "컴포트           레이            일상            칼로리           마시            \n",
      "런자매           투스카니          간식            오늘            남겨            \n",
      "프립            자동차그램         다이어트일기        미숫가루          않고            \n",
      "소백산           스파크           먹스타그램         느낌            보니            \n",
      "사진            페이스북          소이밀크          선식            넣고            \n",
      "운동스타그램        아반테md         씨씨앙           쉐이크           하기            \n",
      "아웃도어엑스크루      순정형가변         온더테이블         까지            되는            \n",
      "이벤트           카마루           너무            코코넛           만들어           \n",
      "런스타그램         맛집            식단기록          제품            하시            \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "intake.LDA = SB_LDA()\n",
    "intake.LDA.make_lda(intake.morph_joined, ntopic=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labnosh.LDA = SB_LDA()\n",
    "labnosh.LDA.make_lda(labnosh.morph_joined, ntopic=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그 토픽과 관련된 문서 10개를 보여준다. 두번째 인자는 토픽의 인덱스이고 세번째 인덱스는 보여줄 포스팅의 개수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic0 = intake.LDA.related_doc(intake.raw_data[:,2], 0, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic0 = labnosh.LDA.related_doc(labnosh.raw_data[:,2], 0, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 형식은 같고 할 줄 알겠지?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake.SB_word2vec = SB_Word2Vec(intake.nav_list)\n",
    "\n",
    "intake.flavor = intake.SB_word2vec.get_sim_sen('맛', intake.raw_data[:,2], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labnosh.SB_word2vec = SB_Word2Vec(labnosh.nav_list)\n",
    "\n",
    "labnosh.flavor = labnosh.SB_word2vec.get_sim_sen('맛', labnosh.raw_data[:,2], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('맛은', 0.6681041717529297),\n",
       " ('식감', 0.6280479431152344),\n",
       " ('생각보다', 0.6160725355148315),\n",
       " ('새콤달콤', 0.6123064756393433),\n",
       " ('미숫가루', 0.5867149829864502),\n",
       " ('단맛', 0.5775174498558044),\n",
       " ('단점', 0.5737367868423462),\n",
       " ('고소', 0.5660829544067383),\n",
       " ('조금', 0.5660343170166016),\n",
       " ('약간', 0.5656855702400208)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intake.SB_word2vec.em.wv.most_similar('맛') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most_similar(positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)¶\n",
    "\n",
    "model.wv.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "[('queen', 0.50882536), ...]\n",
    "\n",
    "model.wv.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])\n",
    "[('queen', 0.71382287), ...]\n",
    "\n",
    "\n",
    "model.wv.doesnt_match(\"breakfast cereal dinner lunch\".split())\n",
    "'cereal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('미숫가루', 0.5826406478881836),\n",
       " ('생각보다', 0.579865574836731),\n",
       " ('심지어', 0.5629949569702148),\n",
       " ('귀리', 0.5474948883056641),\n",
       " ('완전', 0.5468454957008362),\n",
       " ('아주', 0.5345592498779297),\n",
       " ('맛나', 0.5310249328613281),\n",
       " ('그냥', 0.5308135747909546),\n",
       " ('일단', 0.5180487036705017),\n",
       " ('좋아', 0.5167618989944458)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intake.SB_word2vec.em.wv.most_similar(positive=['아침', '맛'], negative=['운동'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF에는 merge 된 리스트를 넣어서 비교해주면 된다. \n",
    "## intake만 있어서 패스\n",
    "## 토픽모델링 된 애들끼리 비교해보는 것도 좋을 듯."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('흡기', 0.25419146503600865),\n",
      " ('튜닝', 0.23128150466795636),\n",
      " ('좋은', 0.2160081977559215),\n",
      " ('파워젤부스트', 0.18764348491928534),\n",
      " ('카스타그램', 0.1800068314632679),\n",
      " ('자동차', 0.17346112850096726),\n",
      " ('아미노리커버', 0.1680063760323834),\n",
      " ('먹으면', 0.16364257405751628),\n",
      " ('단팥죽', 0.13418691072716335),\n",
      " ('머플러', 0.13091405924601301)]\n",
      "\n",
      "[('으로', 0.4224923468565074),\n",
      " ('쇼콜라', 0.34686938659387423),\n",
      " ('미식당', 0.3169824681667258),\n",
      " ('많이', 0.22822737708004256),\n",
      " ('맛이', 0.19471780187384585),\n",
      " ('그래놀라요거트', 0.18384983153670095),\n",
      " ('자색고구마', 0.18203850314717682),\n",
      " ('푸드쉐이크', 0.17162336490741298),\n",
      " ('그린씨리얼', 0.16664221183622158),\n",
      " ('함께', 0.13856662179859727)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf = SB_Tfidf([intake.nav_merged, labnosh.nav_merged])\n",
    "tfidf.get_tfidf()\n",
    "tfidf_of_all = tfidf.tfidf_hangul\n",
    "\n",
    "for i in tfidf_of_all:\n",
    "    pprint(i[:10])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abys\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "labnosh = Social_analysis('labnosh.txt', 'labnosh.official')\n",
    "\n",
    "meals = Social_analysis('meals.txt', 'intakefoods')\n",
    "\n",
    "morningjuk = Social_analysis('morningjuk.txt', 'intakefoods')\n",
    "\n",
    "easy_food = Social_analysis('easy_food.txt')\n",
    "\n",
    "conven_food = Social_analysis('conven_food.txt')\n",
    "\n",
    "keywords = [intake, labnosh, meals, morningjuk, easy_food, conven_food]\n",
    "\n",
    "### 빈도분석\n",
    "\n",
    "for i in keywords:\n",
    "    print(len(i.main_text_list), len(i.morph_merged))\n",
    "\n",
    "labnosh.morph_frequency = analyzer.frequency(labnosh.morph_merged)\n",
    "pprint(labnosh.morph_frequency)\n",
    "\n",
    "meals.morph_frequency = analyzer.frequency(meals.morph_merged)\n",
    "pprint(meals.morph_frequency)\n",
    "\n",
    "morningjuk.morph_frequency = analyzer.frequency(morningjuk.morph_merged)\n",
    "pprint(morningjuk.morph_frequency)\n",
    "\n",
    "easy_food.morph_frequency = analyzer.frequency(easy_food.morph_merged)\n",
    "pprint(easy_food.morph_frequency)\n",
    "\n",
    "conven_food.morph_frequency = analyzer.frequency(conven_food.morph_merged)\n",
    "pprint(conven_food.morph_frequency)\n",
    "\n",
    "### 토픽모델링\n",
    "\n",
    "intake.LDA = analyzer.SB_LDA()\n",
    "intake.LDA.make_lda(intake.morph_joined, ntopic=10)\n",
    "\n",
    "topic0 = intake.LDA.related_doc(intake.main_text_list, 0)\n",
    "\n",
    "topic1 = intake.LDA.related_doc(intake.main_text_list, 1)\n",
    "\n",
    "topic2 = intake.LDA.related_doc(intake.main_text_list, 2)\n",
    "\n",
    "topic3 = intake.LDA.related_doc(intake.main_text_list, 3)\n",
    "\n",
    "topic7 = intake.LDA.related_doc(intake.main_text_list, 7)\n",
    "\n",
    "topic9 = intake.LDA.related_doc(intake.main_text_list, 9)\n",
    "\n",
    "topic3 = intake.LDA.related_doc(intake.main_text_list, 3)\n",
    "\n",
    "\n",
    "\n",
    "labnosh.LDA = analyzer.SB_LDA()\n",
    "labnosh.LDA.make_lda(labnosh.morph_joined, ntopic=10)\n",
    "\n",
    "meals.LDA = analyzer.SB_LDA()\n",
    "meals.LDA.make_lda(meals.morph_joined, ntopic=10)\n",
    "\n",
    "morningjuk.LDA = analyzer.SB_LDA()\n",
    "morningjuk.LDA.make_lda(morningjuk.morph_joined, ntopic=10)\n",
    "\n",
    "easy_food.LDA = analyzer.SB_LDA()\n",
    "easy_food.LDA.make_lda(easy_food.morph_joined, ntopic=10)\n",
    "\n",
    "conven_food.LDA = analyzer.SB_LDA()\n",
    "conven_food.LDA.make_lda(conven_food.morph_joined, ntopic=10)\n",
    "\n",
    "### Word2Vec\n",
    "\n",
    "intake.SB_word2vec = analyzer.SB_Word2Vec(intake.hashtags_appended)\n",
    "\n",
    "intake.flavor = intake.SB_word2vec.get_sim_sen('맛', intake.main_hash_dic, intake.main_text_list, 4)\n",
    "\n",
    "\n",
    "intake.diet = intake.SB_word2vec.get_sim_sen('다이어트', intake.main_hash_dic, intake.main_text_list, 4)\n",
    "\n",
    "\n",
    "intake.health = intake.SB_word2vec.get_sim_sen('건강', intake.main_hash_dic, intake.main_text_list, 4)\n",
    "\n",
    "\n",
    "intake.exercise = intake.SB_word2vec.get_sim_sen('운동', intake.main_hash_dic, intake.main_text_list, 4)\n",
    "\n",
    "\n",
    "intake.morningjuk = intake.SB_word2vec.get_sim_sen('모닝죽', intake.main_hash_dic, intake.main_text_list, 4)\n",
    "\n",
    "\n",
    "intake.meals = intake.SB_word2vec.get_sim_sen('밀스', intake.main_hash_dic, intake.main_text_list, 4)\n",
    "\n",
    "\n",
    "intake.gonyak = intake.SB_word2vec.get_sim_sen('곤약젤리', intake.main_hash_dic, intake.main_text_list, 4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### TFIDF\n",
    "\n",
    "tfidf = analyzer.SB_Tfidf([intake.morph_merged, labnosh.morph_merged, meals.morph_merged, morningjuk.morph_merged, easy_food.morph_merged, conven_food.morph_merged])\n",
    "\n",
    "tfidf_of_all = tfidf.tfidf_hangul\n",
    "\n",
    "for i in tfidf_of_all:\n",
    "    pprint(i[:10])\n",
    "    print()\n",
    "\n",
    "meals_juk_SB_Tfidf = analyzer.SB_Tfidf([meals.morph_merged, morningjuk.morph_merged])\n",
    "\n",
    "tfidf_meals_juk = meals_juk_SB_Tfidf.tfidf_hangul\n",
    "\n",
    "for i in tfidf_meals_juk:\n",
    "    pprint(i[:10])\n",
    "    print()\n",
    "\n",
    "intake_meals_SB_Tfidf = analyzer.SB_Tfidf([intake.morph_merged, meals.morph_merged])\n",
    "\n",
    "tfidf_intake_meals = intake_meals_SB_Tfidf.tfidf_hangul\n",
    "\n",
    "for i in tfidf_intake_meals:\n",
    "    pprint(i[:10])\n",
    "    print()\n",
    "\n",
    "intake_morningjuk_SB_Tfidf = analyzer.SB_Tfidf([intake.morph_merged, morningjuk.morph_merged])\n",
    "\n",
    "tfidf_intake_morningjuk = intake_morningjuk_SB_Tfidf.tfidf_hangul\n",
    "\n",
    "for i in tfidf_intake_morningjuk:\n",
    "    pprint(i[:10])\n",
    "    print()\n",
    "\n",
    "meals_conven_food_SB_Tfidf = analyzer.SB_Tfidf([meals.morph_merged, conven_food.morph_merged])\n",
    "\n",
    "tfidf_meals_conven_food = meals_conven_food_SB_Tfidf.tfidf_hangul\n",
    "\n",
    "for i in tfidf_meals_conven_food:\n",
    "    pprint(i[:10])\n",
    "    print()\n",
    "\n",
    "morningjuk_conven_food_SB_Tfidf = analyzer.SB_Tfidf([morningjuk.morph_merged, conven_food.morph_merged])\n",
    "\n",
    "tfidf_morningjuk_conven_food = morningjuk_conven_food_SB_Tfidf.tfidf_hangul\n",
    "\n",
    "for i in tfidf_meals_conven_food:\n",
    "    pprint(i[:10])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('morph_list.csv', 'w', encoding='utf-8') as f:\n",
    "    spamwriter = csv.writer(f, delimiter=' ',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    for i in intake.morph_list:\n",
    "        spamwriter.writerow(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morph_pos(text_list, add_list, del_list, exception_list = ['맛', '밥', '물', '몸']):\n",
    "    morph_list = []\n",
    "    noun_list = []\n",
    "    adj_list = []\n",
    "    verb_list = []\n",
    "\n",
    "    for j in text_list:\n",
    "        parsed = self.twitter.pos(j)\n",
    "        temp = []\n",
    "        n_temp = []\n",
    "        adj_temp = []\n",
    "        verb_temp = []\n",
    "\n",
    "        for i in parsed:\n",
    "            if self.isHangul(i):\n",
    "                if not i[0] in del_list:\n",
    "                    if len(i[0]) > 1:\n",
    "                        temp.append(i)\n",
    "                        if i[1] == 'Noun':\n",
    "                            n_temp.append(i[0])\n",
    "                        elif i[1] == 'Verb':\n",
    "                            n_adj.append(i[0])\n",
    "                        elif i[1] == 'Adjective':\n",
    "                            n_verb.append(i[0])\n",
    "\n",
    "                    elif i in exception_list:\n",
    "                        temp.append(i)\n",
    "            else: print(i, '한글이 아닙니다.')\n",
    "\n",
    "        morph_list.append(temp)\n",
    "        noun_list.append(n_temp)\n",
    "        adj_list.append(adj_temp)\n",
    "        verb_list.append(verb_temp)\n",
    "\n",
    "\n",
    "    nav_list = noun_list + adj_list + verb_list\n",
    "\n",
    "    return morph_list, nav_list, noun_list, adj_list, verb_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    '''def get_from_dic(self, filename):\n",
    "        self.data = handler.load_by_pickle(filename)\n",
    "        self.dataset, self.dataset_dic, self.main_text_list, self.morph_list, self.morph_merged, self.morph_joined, self.hashtags_merged, self.hashtags_appended, self.main_hash_dic = handler.create_datasets_with_dic(self.data)\n",
    "\n",
    "        # dataset, main_text_list, morph_list, morph_merged, morph_joined, hashtags_merged, hashtags_appended, main_hash_dic\n",
    "    def get_data_from_list(self, filename, add_list, del_list):\n",
    "        self.data = handler.load_by_pickle(filename)\n",
    "        self.dataset, self.main_text_list, self.morph_list, self.morph_merged, self.morph_joined, self.hashtags_merged, self.hashtags_appended, self.main_hash_dic = handler.create_datasets_with_list(self.data[1:], add_list, del_list)'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
