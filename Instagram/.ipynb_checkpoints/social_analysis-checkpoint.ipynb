{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abys\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from ckonlpy.tag import Twitter\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.corpora import Dictionary\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import mglearn\n",
    "from pprint import pprint\n",
    "from ckonlpy.tag import Twitter\n",
    "import numpy as np\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "class Social_analysis():\n",
    "    \n",
    "    non_bmp_map = dict.fromkeys(range(0x10000, sys.maxunicode + 1), 0xfffd)\n",
    "\n",
    "    def __init__(self):\n",
    "        self.twitter = Twitter()\n",
    "        \n",
    "    def pickle_to_table(self, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        data = data[1:]\n",
    "        for idx, i in enumerate(data):\n",
    "            data[idx][2] = i[2].replace('#', ' ').translate(self.non_bmp_map)\n",
    "            data[idx][3] = '/'.join(i[3])\n",
    "            data[idx][4] = '/'.join(i[4])\n",
    "        self.raw_data = np.array(data)\n",
    "  \n",
    "    def hashtags_split(self, hashtags):        \n",
    "        hashtags_split = []\n",
    "        for i in hashtags:\n",
    "            hashtags_split.append(i.split('/'))\n",
    "        \n",
    "        hashtags_list = []\n",
    "        \n",
    "        for i in hashtags_split:\n",
    "            temp = []\n",
    "            for j in i:\n",
    "                if self.isHangul(j):\n",
    "                    t_hashtags = j.translate(self.non_bmp_map)\n",
    "                    temp.append(t_hashtags)\n",
    "            hashtags_list.append(temp)\n",
    "        self.hashtags_list = hashtags_list\n",
    "        \n",
    "        return hashtags_list\n",
    "                    \n",
    "    def add_keyword_dic(self, keyword_list, tag='Noun'):\n",
    "        for i in keyword_list:\n",
    "            if type(i) == tuple:\n",
    "                self.twitter.add_dictionary(i[0], i[1])\n",
    "            else:\n",
    "                self.twitter.add_dictionary(i, tag)\n",
    "        \n",
    "    def morph_pos(self, text_list, exception_list = ['맛', '밥', '물', '몸']):\n",
    "        \n",
    "        morph_list = []\n",
    "        noun_list = []\n",
    "        adj_list = []\n",
    "        verb_list = []\n",
    "        \n",
    "        for j in text_list:\n",
    "            parsed = self.twitter.pos(j)\n",
    "            temp = []\n",
    "            n_temp = []\n",
    "            adj_temp = []\n",
    "            verb_temp = []\n",
    "            \n",
    "            for i in parsed:\n",
    "                if self.isHangul(i[0]):\n",
    "                    if ((len(i[0]) > 1) or (i[0] in exception_list)):\n",
    "                        temp.append(i)\n",
    "                        if i[1] == 'Noun':\n",
    "                            n_temp.append(i[0])\n",
    "                        elif i[1] == 'Verb':\n",
    "                            adj_temp.append(i[0])\n",
    "                        elif i[1] == 'Adjective':\n",
    "                            verb_temp.append(i[0])\n",
    "                    else:\n",
    "                        print('{} 제외'.format(i[0]))\n",
    "                else: print('{} 한글이 아님.'.format(i[0]))\n",
    "            \n",
    "\n",
    "            morph_list.append(temp)\n",
    "            noun_list.append(n_temp)\n",
    "            adj_list.append(adj_temp)\n",
    "            verb_list.append(verb_temp)\n",
    "\n",
    "        nav_list = noun_list + adj_list + verb_list\n",
    "\n",
    "        return morph_list, nav_list, noun_list, adj_list, verb_list\n",
    "\n",
    "    def merge_list(self, tokenized_list):\n",
    "        return [j for i in tokenized_list for j in i]\n",
    "\n",
    "    \n",
    "    def join_list(self, tokenized_list):\n",
    "        joined_list = []\n",
    "        for idx, i in enumerate(tokenized_list):\n",
    "            joined_list.append(\" \".join(i))\n",
    "        return joined_list\n",
    " \n",
    "    def split_list(self, untokenized_list):\n",
    "        hashtag_splited = []\n",
    "        for idx, i in enumerate(untokenized):\n",
    "            hashtag_splited.append(i.split('/'))\n",
    "            return hastag_splited\n",
    "\n",
    "    def word_substitute(self, dataset, sublist):\n",
    "        dataset = copy.deepcopy(dataset)\n",
    "        sub_book = dict()\n",
    "        for i in sublist:\n",
    "            for j in i['sub_words']:\n",
    "                sub_book[j] = i['main']\n",
    "        gc.collect()\n",
    "        for n, i in enumerate(dataset):\n",
    "            dataset[n] = [sub_book.get(item,item) for item in i]\n",
    "\n",
    "        del sub_book\n",
    "        gc.collect()\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    def word_delete(self, dataset, del_list):\n",
    "        dataset = copy.deepcopy(dataset)\n",
    "\n",
    "        for n, line in enumerate(dataset):\n",
    "             dataset[n] = [i for i in line if i not in del_list]\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    \n",
    "    def isHangul(self, text):\n",
    "        encText = text\n",
    "        hanCount = len(re.findall(u'[\\u3130-\\u318F\\uAC00-\\uD7A3]+', encText))\n",
    "        return hanCount > 0\n",
    "\n",
    "    \n",
    "\n",
    "class SB_Word2Vec():    \n",
    "    \n",
    "    def __init__(self, morph_list):\n",
    "        self.dct = Dictionary(morph_list)\n",
    "        self.corpus = [self.dct.doc2bow(line) for line in morph_list]\n",
    "        self.build_Word2Vec(morph_list)\n",
    "    \n",
    "    def make_Word2Vec(self, morph_list, size=50, window=2, min_count=10, iteration=100):\n",
    "        self.em = Word2Vec(morph_list, size=size, window=window, min_count=min_count, iter=iteration)\n",
    "        self.em_vocab = list(self.em.wv.vocab.keys())\n",
    "        self.em_vocab_dic = {word:idx for idx, word in enumerate(self.em_vocab)}\n",
    "\n",
    "    def make_Word2Sen_matrix(self): \n",
    "        vocab_size = len(self.em_vocab)\n",
    "        self.sen_matrix = np.zeros((len(self.corpus), vocab_size))\n",
    "        for idx, row in enumerate(self.sen_matrix):\n",
    "            for idx2, frequency in self.corpus[idx]:\n",
    "                    if self.dct[idx2] in self.em_vocab:\n",
    "                        self.sen_matrix[idx][self.em_vocab_dic[self.dct[idx2]]] = frequency                \n",
    "        self.sim_matrix = np.zeros((vocab_size, vocab_size))\n",
    "        for idx, w1 in enumerate(self.em_vocab):\n",
    "            for idx2, w2 in enumerate(self.em_vocab):\n",
    "                self.sim_matrix[idx][idx2] =  self.em.wv.similarity(w1, w2)\n",
    "\n",
    "        self.word2sen_matrix = np.dot(self.sim_matrix, np.transpose(self.sen_matrix))\n",
    "\n",
    "        return self.word2sen_matrix\n",
    "\n",
    "    def get_sim_sen(self, keyword, main_text, number=1):\n",
    "        self.sim_sen_index = np.argsort(self.word2sen_matrix[self.em_vocab_dic[keyword]])\n",
    "        self.most_sim_sen_index = np.argmax(self.word2sen_matrix[self.em_vocab_dic[keyword]])\n",
    "        index_list = self.sim_sen_index.reshape((-1,)).tolist()\n",
    "        index_list.reverse()\n",
    "        \n",
    "        for idx, i in enumerate(index_list[:number]):\n",
    "            print(str(idx + 1))\n",
    "            print(main_text[i])\n",
    "        return index_list\n",
    "    \n",
    "    def build_Word2Vec(self, morph_list):\n",
    "        self.make_Word2Vec(morph_list)\n",
    "        self.make_Word2Sen_matrix()\n",
    "        \n",
    "        \n",
    "class SB_LDA():\n",
    "\n",
    "    def make_lda(self, morph_joined, ntopic=10, learning_method='batch', max_iter=25, random_state=0, n_words=20):        \n",
    "        self.vect = CountVectorizer(max_features=10000, max_df=.15)\n",
    "        self.X = self.vect.fit_transform(morph_joined)\n",
    "        self.lda = LatentDirichletAllocation(n_components=ntopic, learning_method=learning_method, max_iter=max_iter, random_state=random_state)\n",
    "        self.document_topics = self.lda.fit_transform(self.X)\n",
    "        self.sorting = np.argsort(self.lda.components_, axis=1)[:, ::-1]\n",
    "        self.feature_names = np.array(self.vect.get_feature_names())\n",
    "        mglearn.tools.print_topics(topics=range(ntopic), feature_names=self.feature_names, sorting=self.sorting, topics_per_chunk=5, n_words=n_words)\n",
    "\n",
    "    def related_doc(self, main_text_list, topic_index, number=10):\n",
    "        category = np.argsort(self.document_topics[:, topic_index])[::-1]\n",
    "        related_docs = []\n",
    "        for i in category[:number]:\n",
    "            print(i)\n",
    "            print(main_text_list[i] + \".\\n\")\n",
    "            related_docs.append((i, main_text_list[i]))\n",
    "        return related_docs\n",
    "\n",
    "class SB_Tfidf():    \n",
    "    \n",
    "    def __init__(self, list_morph_merged):\n",
    "        self.list_morph_merged = list_morph_merged\n",
    "        self.dct = Dictionary(self.list_morph_merged)\n",
    "        self.corpus = [self.dct.doc2bow(line) for line in self.list_morph_merged]\n",
    "\n",
    "    def get_tfidf(self):       \n",
    "        self.model = TfidfModel(self.corpus)\n",
    "        self.tfidf = []\n",
    "        for i in self.corpus:\n",
    "             self.tfidf.append(sorted(self.model[i], key = lambda x: x[1], reverse=True))\n",
    "        self.tfidf_hangul = []\n",
    "        for idx1, i in enumerate(self.tfidf):\n",
    "            self.tfidf_hangul.append([(self.dct[j[0]], j[1]) for j in i])        \n",
    "        \n",
    "        return self.tfidf_hangul\n",
    "    \n",
    "def frequency(merged):\n",
    "    word_count = Counter(merged)\n",
    "    word_count2 = []\n",
    "    for i in word_count:\n",
    "        word_count2.append((i, word_count[i]))\n",
    "    word_count2 = sorted(word_count2, key=lambda x: x[1], reverse = True)\n",
    "    return word_count2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake = Social_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB에서 불러온 데이터를 Social_analysis 인스턴스에 등록해도  되고,  그냥 그 데이터로 앞으로 진행해도 된다. 일단 튜토리얼은 pickle 데이터로 진행하기 때문에 아래 함수를 실행하면 intake.row_data 에 저장이 된다. 이 놈은 DB에 들어간 놈이 비슷하게 생긴 놈이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake.pickle_to_table('Data/intake_list.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석에 꼭 포함하고 싶은 단어를 추가한다. 기본적으로는 hashtag들을 noun으로 추가한다. 여기서 해쉬태그 중에 포함하고 싶지 않은 것을 빼는 것을 추천한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5235,)\n",
      "['아침식사', '죽', '모닝죽', '단호박', '인테이크', '맛스타그램', '먹스타그램', '식사대용', '다이어트', '두통', '좋아요반사', '맛집', '간편식', '아점', '아픔', '선팔은맞팔', '맛스타', '선팔', '맞팔', '먹스타', '음식', '푸드스타그램', '좋반']\n"
     ]
    }
   ],
   "source": [
    "hashtag_splited = intake.hashtags_split(intake.raw_data[:, 3])\n",
    "print(np.shape(hashtag_splited))\n",
    "print(hashtag_splited[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## intake.add_dic_keyword() 함수는 1차원 list를 받는다.\n",
    "## 데이터의 형태는 ['단어', '단어', '단어'] or [('단어', Tag), ('단어', Tag)]\n",
    "## Tag는 'Noun', 'Verb', 'Adjective' 등이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_merged = intake.merge_list(hashtag_splited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake.add_keyword_dic(hashtag_splited)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석을 진행한다. 한글자는 제외시키는데 혹시 필요하다고 판단되는 글자는 exception_list 파라미터에 리스트로 넣어주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_list, nav_list, noun_list, adj_list, verb_list= intake.morph_pos(intake.raw_data[:, 2], exception_list=['맛', '밥', '물', '몸', '죽'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_splited = intake.hashtags_split(intake.raw_data[:, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_list = [morph_list, nav_list, noun_list, adj_list, verb_list] + hashtag_splited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 빈도분석에는 merge된 list가 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nav_merged = intake.merge_list(nav_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'맛' in nav_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nav_frequency = frequency(nav_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('인테이크', 5727), ('다이어트', 2167), ('밀스', 1490), ('아침', 1273), ('모닝죽', 1212)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nav_frequency [:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 여기서 내가 형태소분석시에 안짤리게 할 단어와 제외하고 싶은 단어, 대체하고 싶은 단어를 선택한다.\n",
    "## 안짤리게 할 단어는 다시 위로 돌아가 추가한다.\n",
    "## 대체하고 싶은 단어와 제외하고 싶은 단어는 아래 방법으로 처리한다. 일단 생략한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토픽모델링은 조인된 것이 필요하다.\n",
    "### join_list 를 통해서 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_joined = intake.join_list(nav_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파라미터는 다음과 같다. \n",
    "#### self.make_lda(self, morph_joined, ntopic=10, learning_method='batch', max_iter=25, random_state=0, n_words=20)\n",
    "#### ntopic과 n_words를 넣어주면 몇개의 토픽으로 나눌지, 몇개의 단어를 보여줄지를 정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       topic 3       topic 4       \n",
      "--------      --------      --------      --------      --------      \n",
      "모닝죽           힘내            있는            다이어트          건강            \n",
      "아침            운동            많은            저녁            다이어트          \n",
      "일상            시켜            입니            다이어터          구매            \n",
      "먹스타그램         않아            있어            점심            먹스타그램         \n",
      "다이어트          먹겠            같아            들어오시          맛스타그램         \n",
      "단호박           타서            이벤트           다이어트그램        링크            \n",
      "간식            주문했           씨씨앙           1개            데일리           \n",
      "고구마           않는            아니            먹고            유산균           \n",
      "데일리           씹어            괜찮            받은            건강식           \n",
      "맛스타그램         드시            없는            사과            후기            \n",
      "소통            찍어            있다            함께하는          선물            \n",
      "단호박죽          정모            참여            고구마           간식            \n",
      "식단조절          들어            당첨            아침            프로필           \n",
      "아침식사          다노            다이어트          간식            맛있는           \n",
      "운동            직장인           예쁜            다이어트식단        슈퍼바           \n",
      "아침밥           들어가           같다            오늘의식단         비타민           \n",
      "맞팔            버리            같은            오트밀           견과류           \n",
      "모닝            환영            네이버           기록            식단            \n",
      "간편식           채워            있고            삶은계란          일상            \n",
      "모닝죽단호박        하세            그런            식단일기          푸드스타그램        \n",
      "\n",
      "\n",
      "topic 5       topic 6       topic 7       topic 8       topic 9       \n",
      "--------      --------      --------      --------      --------      \n",
      "파워젤부스트        흡기            다이어트          밀스            먹어            \n",
      "아미노리커버        튜닝            다이어터          식사대용          먹었            \n",
      "운동            카스타그램         다이어트식단        다이어트          먹으            \n",
      "맛있            자동차           식단일기          미래식사          먹는            \n",
      "파워젤           문의            점심            우유            챙겨            \n",
      "현정투어          세븐            저녁            너무            먹고            \n",
      "운동하는여자        전화            다이어트그램        식사            감사합           \n",
      "운동하는직장인       머플러           식단            아침            들어            \n",
      "없다            동영상           아침            에서            넣어            \n",
      "러닝            가변배기          두유            밀스라이트         하는            \n",
      "몸스터즈          라이트           오늘            해서            해야            \n",
      "컴포트           레이            일상            칼로리           마시            \n",
      "런자매           투스카니          간식            오늘            남겨            \n",
      "프립            자동차그램         다이어트일기        미숫가루          않고            \n",
      "소백산           스파크           먹스타그램         느낌            보니            \n",
      "사진            페이스북          소이밀크          선식            넣고            \n",
      "운동스타그램        아반테md         씨씨앙           쉐이크           하기            \n",
      "아웃도어엑스크루      순정형가변         온더테이블         까지            되는            \n",
      "이벤트           카마루           너무            코코넛           만들어           \n",
      "런스타그램         맛집            식단기록          제품            하시            \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "intake.LDA = SB_LDA()\n",
    "intake.LDA.make_lda(morph_joined, ntopic=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그 토픽과 관련된 문서 10개를 보여준다. 두번째 인자는 토픽의 인덱스이고 세번째 인덱스는 보여줄 포스팅의 개수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n",
      "d.myday_1027�2018.06.26�\n",
      "아침 : 곤약젤리\n",
      "점심 : 현미밥, 부대찌개\n",
      "저녁 : 곤약젤리\n",
      "간식 : 참외1개, 순살치킨꼬치\n",
      "-\n",
      "오늘부터 장마래서 아침 일찍 운동 나가려고 했는데 이미 장마는 시작되었어욥. 덕분에 얼리버드로 간만에 8시에 아침을 먹었어욥. 다 좋은데 배가 너무 빨리 고파져욥.\n",
      "-\n",
      "오늘 갑자기 먹신님이 오셔서 병원 다녀오는 길에 편의점을 들렸어욥. 부대찌개만 사려고 했는데 포스기 옆에 치킨들이 나를 불렀어욥. 결심했어욥. 우리집에 데려가기로. 그 장맛비를 뚫고 무사히 사와서 무사히 헤치웠어욥. 부대찌개는 라면사리만 열심히 건져먹고 햄은 매늬 남았어욥. 내일 먹을거에욥. 점심을 옴총 잘먹었더니 하루종일 배가 불러서 저녁은 가볍게 패스할 수 있을 줄 알았어욥. 정확히 여덟시 오십분에 곤약젤리 꺼냈어욥.\n",
      "-\n",
      "참외는 씹어먹어야 맛나는건줄 알았는데 이번에 사온 참외가 맛이 없는 거였어욥.\n",
      "-\n",
      "➡️ 그나저나 이 비들 실화에욥? 난 비오는 날 외출이 제일 시럽��.\n",
      "\n",
      "4238\n",
      "a1dana1004� 견과류 대부분이 두뇌회전에도 좋고~\n",
      "다이어트, 노화예방등에도 좋다죠~\n",
      "요즘 웰빙시대에\n",
      "견과류는 일부러 챙겨드시는 분들이\n",
      "점점 더 늘고 계신것 같아요!\n",
      "\n",
      "매일매일  하루견과\n",
      "그래서 다가오는  추석선물\n",
      " 견과류선물 좋은것 같아요!\n",
      "\n",
      "남녀노소 누구나할것없이\n",
      "어린이들부터 어르신들까지\n",
      "누구나 즐길 수 있는  선물1순위\n",
      "\n",
      " 인테이크  닥터넛츠\n",
      "선물하기도 좋은 고급스런 패키지\n",
      "\n",
      "괜찮죠?^^  선물세트 추석선물세트  간식스타그램  간식그램  영양간식  간식타임  어린이간식  술안주  맥주안주  안주스타그램 맛있다그램  맛있다  식사대용  먹방스타그램  먹스타그램  건강스타그램  건강그램  건강  가족건강  nuts  gift  건강간식  영양간식.\n",
      "\n",
      "3995\n",
      "love_07.30몸이 무거울땐 가볍게 먹자고 주문한건데_\n",
      "이건뭐 간식수준_ �\n",
      " 식사대용 아침밥대신 가볍게 먹을라한건데\n",
      "맛있잖아 _ 양이 너무 작아 결국 컵라면 뜯었음\n",
      "딸램이도 두봉이나 흡입 _ 더 시키자�\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      " 데일리그램 데일리 먹스타그램 맛스타그램 호박죽 단호박죽 모닝죽 인테이크 로켓배송 쿠팡 맛있다 아침 저녁 다이어트 유지어터 간식 디저트 주부놀이 쇼핑 애스타그램 딸스타그램 럽스타그램 줌마그램 일상 소통 선팔 맞팔.\n",
      "\n",
      "4036\n",
      "sujin_siso하루에 한봉씩 챙겨먹고있는 견과류에요!\n",
      "봉투를 열자마자 퍼지는 고소한향에 반하고\n",
      "맛에 두번 반한 닥터넛츠 �\n",
      "몸이 허해질 수 밖에 없는 계절인데\n",
      "모두 다 같이 닥터넛츠로 건강챙기자구요 �\n",
      "\n",
      " 닥터넛츠  견과류  인테이크  건강  관리  아몬드  호두  피칸  피스타치오  맛있다  맛있다그램  맛스타그램  먹스타그램  다이어트  추천  셀카  셀피  셀스타그램  얼스타그램  렌즈  프레쉬콘  소통  좋아요  맞팔  선팔.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic0 = intake.LDA.related_doc(intake.raw_data[:,2], 0, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 형식은 같고 할 줄 알겠지?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake.SB_word2vec = SB_Word2Vec(nav_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "intakefoods귀차니즘 한방에 해결! 한끼 식사대용으로 굿❤️ 미래식사\n",
      ".\n",
      "후기남겨주신 @sundaenge.dd 님 감사합니다!\n",
      "・・・\n",
      " 아침식단 D+166 160112 아침 6:50\n",
      " 긴글주의  읽으면복받음\n",
      ".\n",
      " 인테이크  선식  밀스  밀스오리지널\n",
      ".\n",
      ".\n",
      "얼마전에 받은 밀스로 오늘 아침은 간단하게�(사실 어제 밤 8시에 케이크먹은�) 밀스에 대한 설명과 후기 작성할께요�\n",
      ".\n",
      ".\n",
      " 인테이크설명 � @intakefoods\n",
      "� 완전한  미래식사 누구나 꿈꿔왔던 빠르고 완전한식사\n",
      ".\n",
      " 구성 (1일영양소기준치에대한비율)\n",
      "탄수화물 57%(33가지곡류채소로구성)\n",
      "단백질 20%(식물성 단백질 대두분리 단백질과 동물성인 유청분리 단백질조합) � 23g\n",
      "지방 15%(아몬드파우더)\n",
      "식이섬유 5% 미네랄 프로바이오틱스 비타민 각1%\n",
      ".\n",
      ".\n",
      " 선댕이후기 �\n",
      "맛은 제가 지금까지 먹어본 선식들보다 맛있어요� 전 처음엔 무조건 물에 타 먹어보거든요 밀스는 물에 타먹어도 거부감 없고 미숫가루 먹는 느낌~!(저의 기준입니다☺) 그리고 안에 뭔가 씹히는게 견과류쪽인거 같은데 마시는 끝까지 씹혀서 먹는재미도 쏠쏠~ 그리고 끼니마다 단백질등 영양성분에 예민한 저에겐 저기 영양성분표를 보고 완전 마음에 들었네요��� 사실 대중에 있는 선식엔 영양성분표가 없는게 너무 아쉬웠거든요�\n",
      "그래서 맛이있어도 저희 오빠 간식용으로 먹이는� 그리고 보틀에 포장되어 있으니 와우� 짱 편함�� 다만 역시나 선식은 비싸서� 오빠랑 이구동성으로 싸면 쟁여놓고 싶다고 말했네요�(선식은 좋은재료가 많이들어가니 어쩔수없고�) 아~! 제가 느끼는 포만감은 10:40분(6시50분먹음) 배가 고파지기시작~ 생각보다 포만감이 오래감� 우유나 두유에 타먹으면 더 오래갈듯☺\n",
      ".\n",
      ".\n",
      " 깨알오빠후기 �\n",
      "지금까지 먹어본거 중 제일 맛있고 씹히는게 많아서 좋아� 포만감도 좋구만~(참고로 저희오빠는 선식을 간식으로 드셔요� 오빠는  소이카페 에 타드심)\n",
      ".\n",
      ".\n",
      "긴글읽어주신 여러분 오늘 행복만땅 지방타파 하루되실꺼에용���\n",
      ".\n",
      ".\n",
      " 1일1넛  다노한한끼 다노샵  다이어트식단  건강식\n",
      " 식사대용  아침식사  데일리\n",
      " 순전히개인적인입맛의후기입니다✋\n",
      " 좋은제품감사드립니다��\n",
      "- 밀스 인테이크 협찬 제품\n",
      ".\n",
      "구매는 프로필링크에서!\n",
      "2\n",
      "intakefoods밥 먹을 시간 없다고 회사에서 식사 거르지 마세요 ! 가방에 인테이크 단팥죽 하나 챙겨가시면 점심 걱정 끝!! 후기를 남겨주신 @milarang 님 감사합니다!!\n",
      "・・・\n",
      "게으름뱅이 새댁에게  할렐루야 !! �\n",
      "회사서 밥나와도 시간없다고 빵먹고 다니는 출근하는 남편손에  단팥죽 살짝 데워  빈스바 와 안겨주면 끝 �  아침식사\n",
      "쪽쪽 알아서 빨아드시겠쥬? ㅋㅋㅋ\n",
      "혼자밥먹기 싫은 저도 간단하고 건강하게 한끼 해결되어서 넘 좋아요 지금 전 간식으로 ㅋㅋ�\n",
      "팥 너무너무 좋아하는데 달지 않아 딱�\n",
      "@intakefoods  인테이크  모닝죽  새댁일상  새댁스타그램 은 아까..��  건강한한끼  웰빙라이프  동탄새댁 �\n",
      ".\n",
      "인테이크 푸즈는 다이어트하시는 분, 입맛이 없으신 분들을 위한 가장 맛있는 건강식을 배달해드립니다!\n",
      ".\n",
      "구매는 프로필 링크><\n",
      ".\n",
      " 인테이크  모닝죽  죽  먹스타그램  맛스타그램  주부  신혼  주부스타그램  플레이팅  건강  다이어트  건강식  존맛  데일리  푸드스타그램  식단  견과류  간식  후식  에너지바  영양바\n",
      "3\n",
      "intakefoods굿모닝입니다!! 아침드실 시간이 없다면 가방에 모닝죽 넣어가기!! 후기를 남겨주신 @hsi26 님 감사합니다!!\n",
      "・・・\n",
      " 굿모닝 good morning\n",
      "@intakefoods  모닝죽단호박\n",
      "월요일 아침은 바쁘게 출근하는 남편 오늘은\n",
      "간편하게  모닝죽 으로\n",
      "색도 곱고 간단하게 먹기 좋으다  세상에서가장간편한아침  단호박죽  아침식사 breakfast  아침밥\n",
      ".\n",
      "인테이크 푸즈는 다이어트하시는 분, 입맛이 없으신 분들을 위한 가장 맛있는 건강식을 배달해드립니다!\n",
      ".\n",
      "구매는 프로필 링크><\n",
      ".\n",
      " 인테이크  모닝죽  죽  먹스타그램  맛스타그램  주부  새댁  신혼  주부스타그램  플레이팅  건강  다이어트  건강식  존맛  데일리  푸드스타그램  식단  견과류  간식  후식  에너지바  영양바\n",
      "4\n",
      "yumyum_daisy당분간 저의 아침을 책임질 간편죽이에요� 처음에 아빠가 구입해줘서 알게됐는데 맛도 좋고 장소 안따지고 먹기에도 편해서 이번엔 제가 직접구입! 세박스 사면 한박스 랜덤증정 이벤트하길래 망설이지 않고 구입했어요! 일단 처음은 고구마죽을 먹었는데 작은 고구마 알갱도 씹히면서 너무 달지도 않고 짱입니다�� 죽이고 양도 과하지 않다보니 배가 생각보다 빨리 고프긴 한데 아침 안먹는것보단 낫잖아요?!� 식간에 출출할때 다른걸로 군것질할바에 이거 먹는것도 좋은것같아요� 이번에는 검은콩죽, 고구마죽 , 가장 최근에 나온 우유죽을 구입하고 랜덤으로 고구마죽을 받았는데 앞으로 하나씩 맛 후기 남겨볼게요~* .\n",
      ".\n",
      ".\n",
      " 먹스타그램  아침  간식 인테이크  간편죽  모닝죽  인테이크모닝죽  고구마  고구마죽  모닝죽고구마  편리  간편  추천  데이지_후기\n"
     ]
    }
   ],
   "source": [
    "intake.flavor = intake.SB_word2vec.get_sim_sen('모닝죽', intake.raw_data[:,2], 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF에는 merge 된 리스트를 넣어서 비교해주면 된다. \n",
    "## intake만 있어서 패스\n",
    "## 토픽모델링 된 애들끼리 비교해보는 것도 좋을 듯."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = SB_Tfidf([intake.morph_merged, labnosh.morph_merged, meals.morph_merged, morningjuk.morph_merged, easy_food.morph_merged, conven_food.morph_merged])\n",
    "\n",
    "tfidf_of_all = tfidf.tfidf_hangul\n",
    "\n",
    "for i in tfidf_of_all:\n",
    "    pprint(i[:10])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abys\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "labnosh = Social_analysis('labnosh.txt', 'labnosh.official')\n",
    "\n",
    "meals = Social_analysis('meals.txt', 'intakefoods')\n",
    "\n",
    "morningjuk = Social_analysis('morningjuk.txt', 'intakefoods')\n",
    "\n",
    "easy_food = Social_analysis('easy_food.txt')\n",
    "\n",
    "conven_food = Social_analysis('conven_food.txt')\n",
    "\n",
    "keywords = [intake, labnosh, meals, morningjuk, easy_food, conven_food]\n",
    "\n",
    "### 빈도분석\n",
    "\n",
    "for i in keywords:\n",
    "    print(len(i.main_text_list), len(i.morph_merged))\n",
    "\n",
    "labnosh.morph_frequency = analyzer.frequency(labnosh.morph_merged)\n",
    "pprint(labnosh.morph_frequency)\n",
    "\n",
    "meals.morph_frequency = analyzer.frequency(meals.morph_merged)\n",
    "pprint(meals.morph_frequency)\n",
    "\n",
    "morningjuk.morph_frequency = analyzer.frequency(morningjuk.morph_merged)\n",
    "pprint(morningjuk.morph_frequency)\n",
    "\n",
    "easy_food.morph_frequency = analyzer.frequency(easy_food.morph_merged)\n",
    "pprint(easy_food.morph_frequency)\n",
    "\n",
    "conven_food.morph_frequency = analyzer.frequency(conven_food.morph_merged)\n",
    "pprint(conven_food.morph_frequency)\n",
    "\n",
    "### 토픽모델링\n",
    "\n",
    "intake.LDA = analyzer.SB_LDA()\n",
    "intake.LDA.make_lda(intake.morph_joined, ntopic=10)\n",
    "\n",
    "topic0 = intake.LDA.related_doc(intake.main_text_list, 0)\n",
    "\n",
    "topic1 = intake.LDA.related_doc(intake.main_text_list, 1)\n",
    "\n",
    "topic2 = intake.LDA.related_doc(intake.main_text_list, 2)\n",
    "\n",
    "topic3 = intake.LDA.related_doc(intake.main_text_list, 3)\n",
    "\n",
    "topic7 = intake.LDA.related_doc(intake.main_text_list, 7)\n",
    "\n",
    "topic9 = intake.LDA.related_doc(intake.main_text_list, 9)\n",
    "\n",
    "topic3 = intake.LDA.related_doc(intake.main_text_list, 3)\n",
    "\n",
    "\n",
    "\n",
    "labnosh.LDA = analyzer.SB_LDA()\n",
    "labnosh.LDA.make_lda(labnosh.morph_joined, ntopic=10)\n",
    "\n",
    "meals.LDA = analyzer.SB_LDA()\n",
    "meals.LDA.make_lda(meals.morph_joined, ntopic=10)\n",
    "\n",
    "morningjuk.LDA = analyzer.SB_LDA()\n",
    "morningjuk.LDA.make_lda(morningjuk.morph_joined, ntopic=10)\n",
    "\n",
    "easy_food.LDA = analyzer.SB_LDA()\n",
    "easy_food.LDA.make_lda(easy_food.morph_joined, ntopic=10)\n",
    "\n",
    "conven_food.LDA = analyzer.SB_LDA()\n",
    "conven_food.LDA.make_lda(conven_food.morph_joined, ntopic=10)\n",
    "\n",
    "### Word2Vec\n",
    "\n",
    "intake.SB_word2vec = analyzer.SB_Word2Vec(intake.hashtags_appended)\n",
    "\n",
    "intake.flavor = intake.SB_word2vec.get_sim_sen('맛', intake.main_hash_dic, intake.main_text_list, 4)\n",
    "\n",
    "\n",
    "intake.diet = intake.SB_word2vec.get_sim_sen('다이어트', intake.main_hash_dic, intake.main_text_list, 4)\n",
    "\n",
    "\n",
    "intake.health = intake.SB_word2vec.get_sim_sen('건강', intake.main_hash_dic, intake.main_text_list, 4)\n",
    "\n",
    "\n",
    "intake.exercise = intake.SB_word2vec.get_sim_sen('운동', intake.main_hash_dic, intake.main_text_list, 4)\n",
    "\n",
    "\n",
    "intake.morningjuk = intake.SB_word2vec.get_sim_sen('모닝죽', intake.main_hash_dic, intake.main_text_list, 4)\n",
    "\n",
    "\n",
    "intake.meals = intake.SB_word2vec.get_sim_sen('밀스', intake.main_hash_dic, intake.main_text_list, 4)\n",
    "\n",
    "\n",
    "intake.gonyak = intake.SB_word2vec.get_sim_sen('곤약젤리', intake.main_hash_dic, intake.main_text_list, 4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### TFIDF\n",
    "\n",
    "tfidf = analyzer.SB_Tfidf([intake.morph_merged, labnosh.morph_merged, meals.morph_merged, morningjuk.morph_merged, easy_food.morph_merged, conven_food.morph_merged])\n",
    "\n",
    "tfidf_of_all = tfidf.tfidf_hangul\n",
    "\n",
    "for i in tfidf_of_all:\n",
    "    pprint(i[:10])\n",
    "    print()\n",
    "\n",
    "meals_juk_SB_Tfidf = analyzer.SB_Tfidf([meals.morph_merged, morningjuk.morph_merged])\n",
    "\n",
    "tfidf_meals_juk = meals_juk_SB_Tfidf.tfidf_hangul\n",
    "\n",
    "for i in tfidf_meals_juk:\n",
    "    pprint(i[:10])\n",
    "    print()\n",
    "\n",
    "intake_meals_SB_Tfidf = analyzer.SB_Tfidf([intake.morph_merged, meals.morph_merged])\n",
    "\n",
    "tfidf_intake_meals = intake_meals_SB_Tfidf.tfidf_hangul\n",
    "\n",
    "for i in tfidf_intake_meals:\n",
    "    pprint(i[:10])\n",
    "    print()\n",
    "\n",
    "intake_morningjuk_SB_Tfidf = analyzer.SB_Tfidf([intake.morph_merged, morningjuk.morph_merged])\n",
    "\n",
    "tfidf_intake_morningjuk = intake_morningjuk_SB_Tfidf.tfidf_hangul\n",
    "\n",
    "for i in tfidf_intake_morningjuk:\n",
    "    pprint(i[:10])\n",
    "    print()\n",
    "\n",
    "meals_conven_food_SB_Tfidf = analyzer.SB_Tfidf([meals.morph_merged, conven_food.morph_merged])\n",
    "\n",
    "tfidf_meals_conven_food = meals_conven_food_SB_Tfidf.tfidf_hangul\n",
    "\n",
    "for i in tfidf_meals_conven_food:\n",
    "    pprint(i[:10])\n",
    "    print()\n",
    "\n",
    "morningjuk_conven_food_SB_Tfidf = analyzer.SB_Tfidf([morningjuk.morph_merged, conven_food.morph_merged])\n",
    "\n",
    "tfidf_morningjuk_conven_food = morningjuk_conven_food_SB_Tfidf.tfidf_hangul\n",
    "\n",
    "for i in tfidf_meals_conven_food:\n",
    "    pprint(i[:10])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('morph_list.csv', 'w', encoding='utf-8') as f:\n",
    "    spamwriter = csv.writer(f, delimiter=' ',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    for i in intake.morph_list:\n",
    "        spamwriter.writerow(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morph_pos(text_list, add_list, del_list, exception_list = ['맛', '밥', '물', '몸']):\n",
    "    morph_list = []\n",
    "    noun_list = []\n",
    "    adj_list = []\n",
    "    verb_list = []\n",
    "\n",
    "    for j in text_list:\n",
    "        parsed = self.twitter.pos(j)\n",
    "        temp = []\n",
    "        n_temp = []\n",
    "        adj_temp = []\n",
    "        verb_temp = []\n",
    "\n",
    "        for i in parsed:\n",
    "            if self.isHangul(i):\n",
    "                if not i[0] in del_list:\n",
    "                    if len(i[0]) > 1:\n",
    "                        temp.append(i)\n",
    "                        if i[1] == 'Noun':\n",
    "                            n_temp.append(i[0])\n",
    "                        elif i[1] == 'Verb':\n",
    "                            n_adj.append(i[0])\n",
    "                        elif i[1] == 'Adjective':\n",
    "                            n_verb.append(i[0])\n",
    "\n",
    "                    elif i in exception_list:\n",
    "                        temp.append(i)\n",
    "            else: print(i, '한글이 아닙니다.')\n",
    "\n",
    "        morph_list.append(temp)\n",
    "        noun_list.append(n_temp)\n",
    "        adj_list.append(adj_temp)\n",
    "        verb_list.append(verb_temp)\n",
    "\n",
    "\n",
    "    nav_list = noun_list + adj_list + verb_list\n",
    "\n",
    "    return morph_list, nav_list, noun_list, adj_list, verb_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    '''def get_from_dic(self, filename):\n",
    "        self.data = handler.load_by_pickle(filename)\n",
    "        self.dataset, self.dataset_dic, self.main_text_list, self.morph_list, self.morph_merged, self.morph_joined, self.hashtags_merged, self.hashtags_appended, self.main_hash_dic = handler.create_datasets_with_dic(self.data)\n",
    "\n",
    "        # dataset, main_text_list, morph_list, morph_merged, morph_joined, hashtags_merged, hashtags_appended, main_hash_dic\n",
    "    def get_data_from_list(self, filename, add_list, del_list):\n",
    "        self.data = handler.load_by_pickle(filename)\n",
    "        self.dataset, self.main_text_list, self.morph_list, self.morph_merged, self.morph_joined, self.hashtags_merged, self.hashtags_appended, self.main_hash_dic = handler.create_datasets_with_list(self.data[1:], add_list, del_list)'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
